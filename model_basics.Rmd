
# Model basics

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE
)
```

```{r message=FALSE, warning=FALSE}
# Libraries
library(tidyverse)
library(modelr)
library(tidymodels)

# Parameters
file_anscombe <- here::here("data/anscombe/anscombe_1.rds")
file_anscombe_outlier <- here::here("data/anscombe/anscombe_1_outlier.rds")

#===============================================================================

df <- read_rds(file_anscombe)
df_outlier <- read_rds(file_anscombe_outlier)

df_limits_coord <-
  list(
    scale_x_continuous(limits = c(0, 20)),
    scale_y_continuous(limits = c(0, 15)),
    coord_fixed()
  )

label_model_line <- function(error_metric, a_0, a_1) {
  str_glue("{error_metric}\na_0 = {round(a_0, 2)}\na_1 = {round(a_1, 2)}")
}
```

## What is a model?

The world is complicated and messy, and there are endless details to even simple phenomena. To understand and navigate the world, you construct and use models. 

For example, think about traffic. You probably have a simple mental model that says traffic will be worse at rush hour than at two in the morning. You can use this model to describe how traffic varies throughout the day, but you can also use it to predict the level of traffic at a given hour. 

Your mental model of traffic, like any model, is an approximation. It tries to capture relevant information while ignoring noise and less important details. Your traffic mental model will never fully explain traffic, and so you'll never be a perfect predictor of how many cars will be on the road at any given time. However, if it's a good model, it will be useful.

The type of models we'll discuss in this book are similar to your mental model of traffic: they are approximations; you can use them to both describe and predict the world; and, while they will never be completely accurate, they can be useful.

## Function families

In this chapter, we'll introduce the idea of functions as a type of mathematical model. There are other types of mathematical models that aren't functions, but we're not going to cover those in this book. 

Functions map from one variable (e.g., time) to another (e.g., amount of traffic). Because functions have a form that you can write down, you can use them to describe the relationship between variables, but you can also plug values into your function to predict what you don't already know.

Here's the first thing to know about model-building: your job as a modeler is to find the function that best approximates your data. 

There is an infinite number of functions, so how do you know where to look? The first step is to decide on a _function family_. Function families are sets of functions with similar forms. One of the most common function families in modeling is family of linear functions, which all take the following form:

`y = a_0 + a_1 * x`

`x` is your input variable, the variable that you supply to the function in hopes of approximating some other variable. In our traffic example, `x` is the time of day.

`a_0` and `a_1` are the _parameters_. These two numbers define the line. The only difference between distinct functions in the family of linear functions are their values of _a_0_ and _a_1_.

To visualize this, here's a plot of many different lines, each of which has a different combination of `a_0` and `a_1`.

```{r}
tribble(
    ~a_1, ~a_0, #~x, ~y,
    1,    3,    #39, 48,
    5,    -50,  #45, 23.5,
    -.1,  15,   #5,  19,
    -.7,  12,   #11, 48,
    0.1,  4,    #10, 12 
  ) %>% 
  ggplot(aes(x, y)) + 
  #geom_label(aes(label = str_glue("a_0 = {a_0}\na_1 = {a_1}"))) +
  geom_abline(aes(slope = a_1, intercept = a_0)) +
  df_limits_coord
```

`a_0` defines the y-intercept, the y-value your function will produce with `x` is 0. `a_1` is the slope, or how much increase in `y` the function says results from a one unit increase in `x`. 

In order to determine which function best approximates the relationship you're trying to model, you first determine which function family best describes your data. We'll talk more about visualizing your data to determine functional form in the next chapter, when we talk about exploratory data analysis. For now, just now that, if the relationship looks linear, it's a good idea to use a linear function to approximate your data. 

Because linear functions are defined by their two parameters, _a_0_ and _a_1_, your task becomes figuring out which values of _a_0_ (the intercept) and _a_1_ (the slope) to use. In the next section, we'll talk about how to find these two parameters.  

## Fitting a model

[Anscombe's quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet) is a set of four small data sets. Each of the four data set can be described by the same linear function, but they are visually very different. In this section, we'll use a modified version of the first data set in the quartet. 

```{r}
df %>% 
  knitr::kable()
```

### Decide on a function family

Recall that we said the first step to fitting a model involves determining the functional form of your data. Let's visualize the relationship between `x` and `y`.

```{r}
df %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  df_limits_coord
```

The relationship between `x` and `y` looks linear, so a function in the linear family will likely make a good approximation. 

### Decide on an error metric

Now, we need a way of determining which linear function best approximates the relationship between `x` and `y`. There are many different functions we could use.

```{r}
ab_lines <-
  tribble(
    ~intercept, ~slope,
    -1.5,       1,
    3,          0.5,
    0.75,       0.75,
    3.37,       0.52,
    2.27,       0.51
  ) %>% 
  pmap(~ geom_abline(intercept = ..1, slope = ..2, color = "blue"))
  
df %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  ab_lines +
  df_limits_coord
```

How do we decide which one is best? Let's just pick one of the lines and take a closer look.

```{r}
model_guess <- list(a_0 = -1.5, a_1 = 1)

df %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_abline(
    slope = model_guess$a_1, 
    intercept = model_guess$a_0, 
    color = "blue"
  ) +
  annotate(
    geom = "label",
    x = 17.5, 
    y = 14.6,
    label = str_glue("a_0 = {model_guess$a_0}\na_1 = {model_guess$a_1}"),
    hjust = 0
  ) +
  df_limits_coord
```

By glancing at the plot, you can tell that this function isn't doing the best job of approximating our data. Most of the points where `x < 9` fall above our line, and most of the points where `x > 9` fall below the line.

To compare our chosen line to other possibilities, we need a way of quantifying how well the model approximates our data. A common way to assess model fit involves calculating the distances between the line and each of the data points.

```{r}
df %>%
  mutate(resid = y - (model_guess$a_0 + model_guess$a_1 * x)) %>% 
  ggplot(aes(x, y)) +
  geom_abline(
    slope = model_guess$a_1, intercept = model_guess$a_0, 
    color = "blue"
  ) +
  geom_segment(aes(xend = x, yend = y - resid), linetype = "dotted") +
  geom_point() +
  annotate(
    geom = "label",
    x = 17.3, 
    y = 14.6,
    label = str_glue("a_0 = {model_guess$a_0}\na_1 = {model_guess$a_1}"),
    hjust = 0
  ) +
  df_limits_coord +
  labs(title = "Residuals")
```

These distances are called _residuals_ (or errors). Each residual represents the difference between the `y` value that the model predicts for a given `x` and the actual `y` associated with that `x`. The larger a residual, the worse your model approximates `y` at that given `x`.  

We want to minimize all residuals, so we need to combine the individual distances into a single error metric. One common option is to find the _root mean-square error (RMSE)_. To calculate the RMSE, square each residual, find the mean, and then take the square root of that mean:

```{r eval=FALSE, echo=TRUE}
sqrt(mean(residuals^2))
```

We'll cover the R functions that calculate RMSE in later chapters.

```{r echo=FALSE}
rmse_guess <-
  df %>% 
  mutate(pred = model_guess$a_0 + model_guess$a_1 * x) %>% 
  yardstick::rmse(truth = y, estimate = pred) %>% 
  pull(.estimate)
```

The RMSE of our chosen model is `r rmse_guess`. We could calculate the RMSE of each model we originally plotted and pick the one with lowest value. However, that method won't necessarily result in finding the model that minimizes RMSE for our data because there is an infinite number of possible models. 

Instead, we'll hand off the work to an algorithm (implemented in an R function) that will return the values of `a_0` and `a_1` that minimize RMSE for our data.

### Find your parameters

To determine which `a_0` and `a_1` minimize RMSE, we'll use the function `lm()`. `lm()` needs two arguments: a function family and your data. Then, it finds the parameters that define the function with the specified family that minimizes RMSE for your data.

In later chapters, you'll learn how to actually carry this out in R. For now, we'll just show you the results.

```{r}
model_lm <- lm(y ~ x, data = df)
a_0_model_lm <- coef(model_lm)[["(Intercept)"]]
a_1_model_lm <- coef(model_lm)[["x"]]
```

Here's the model that `lm()` came up with:

`y = `r a_0_model_lm` + `r a_1_model_lm`x`

And here's the model plotted against our data:

```{r}
df %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_abline(intercept = a_0_model_lm, slope = a_1_model_lm, color = "blue") +
  annotate(
    geom = "label",
    x = 17.75, 
    y = 10.3,
    label = label_model_line("RMSE", a_0_model_lm, a_1_model_lm),
    hjust = 0
  ) +
  df_limits_coord 
```

The RMSE of the model found with `lm()` is `r modelr::rmse(model_lm, df)`, and you can see from the visualization that this model does a much better job of approximating our data than our previous attempt.

Again, `a_0` is the intercept, so we know that our model predicts that `y` will be `r a_0_model_lm` when `x = 0`. `a_1` is the slope, which means that our model predicts a `r a_1_model_lm` increase in `y` each time `x` increases by 1.   

Now, we'll talk a little bit more about error metric choice. 

## Error metric choice

One downside of models that minimize RMSE is that they're very sensitive to outliers. The following plot shows our original data plus an outlier.

```{r}
df_outlier %>%
  ggplot(aes(x, y)) +
  geom_point() +
  geom_point(
    color = "red", 
    data = df_outlier %>% filter(near(y, min(y)))
  ) +
  df_limits_coord +
  labs(title = "Data with outlier")
```

If we keep using RMSE as our error metric, this single point changes the model significantly.

```{r}
model_lm_outlier <- lm(y ~ x, data = df_outlier)
a_0_rmse_outlier <- coef(model_lm_outlier)[["(Intercept)"]]
a_1_rmse_outlier <- coef(model_lm_outlier)[["x"]]

df_outlier %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_abline(
    intercept = a_0_model_lm,
    slope = a_1_model_lm,
    alpha = 0.3
  ) +
  geom_abline(
    intercept = a_0_rmse_outlier, 
    slope = a_1_rmse_outlier,
    color = "blue"
  ) +
  annotate(
    geom = "text", 
    label = "Original model, no outlier",
    x = 14, 
    y = 10.5, 
    hjust = 0,
    angle = 26,
    color = "gray"
  ) +
  annotate(
    geom = "label",
    label = label_model_line("RMSE", a_0_rmse_outlier, a_1_rmse_outlier),
    x = 17,
    y = 4.4,
    hjust = 0
  ) +
  df_limits_coord +
  labs(title = "Data with outlier")
```

The outlier pulls the slope down and the intercept up, and the model no longer approximates the rest of the data well. 

We want to approximate the linear trend present in the rest of the data, so we'll need to find a way to build a better model. There are two options:

* Use a different error metric
* Exclude the outlier from the data

First, we'll try using a different error metric. Because RMSE squares each residual, models that fit most of the data well can still have high RMSE values due to outliers. The large outlier residuals pull the model away from the majority of the data and towards the outliers.

Other error metric, such as _mean absolute error (MAE)_ are less sensitive to outliers. Instead of squaring each residual, MAE finds their absolute values:

```{r eval=FALSE, echo=TRUE}
mean(abs(residuals))
```

We'll fit another model on the outlier data, using MAE as our error metric instead of RMSE.

```{r}
mae <- function(a, data) {
  resid <- data$y - pred(a, data)
  mean(abs(resid))
}

pred <- function(a, data) {
  a[1] + a[2] * data$x 
}

best_mae <- optim(c(0, 0), mae, data = df_outlier)
a_0_mae <- best_mae$par[1]
a_1_mae <- best_mae$par[2]


df_outlier %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_abline(
    intercept = coef(model_lm_outlier)[["(Intercept)"]],
    slope = coef(model_lm_outlier)[["x"]],
    color = "blue"
  ) +
  geom_abline(
    intercept = a_0_mae, 
    slope = a_1_mae,
    color = "#f7766d"
  ) +
  annotate(
    geom = "label", 
    label = label_model_line("MAE", a_0_mae, a_1_mae),
    x = 17.2, 
    y = 14.2, 
    hjust = 0
  ) +
  annotate(
    geom = "label",
    x = 17,
    y = 4.4,
    label = label_model_line("RMSE", a_0_rmse_outlier, a_1_rmse_outlier),
    hjust = 0
  ) +
  df_limits_coord +
  labs(title = "Data with outlier")
```

The MAE model isn't as affected by our outlier. You can't adapt `lm()` to use MAE, but the R function `rlm()` is more flexible, and allows for various different error metrics.

The MAE model is actually very similar to the model that we fit on the original data without the outlier.

```{r}
df_outlier %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_abline(
    intercept = a_0_mae, 
    slope = a_1_mae,
    color = "#f7766d"
  ) +
  geom_abline(
    intercept = a_0_model_lm,
    slope = a_1_model_lm,
    alpha = 0.5
  ) +
  annotate(
    geom = "text", 
    label = "Original model, no outlier",
    x = 14, 
    y = 10.5, 
    hjust = 0,
    angle = 26,
    color = "gray"
  ) +
  annotate(
    geom = "label",
    label = label_model_line("MAE", a_0_mae, a_1_mae),
    x = 17.2, 
    y = 9.4, 
    hjust = 0
  ) +
  df_limits_coord +
  labs(title = "Data with outlier")
```

This brings us to a second approach: remove the outlier. You might wonder why it's okay to change the data before fitting your model. Couldn't you use this process to produce any model you wanted? You could, but that's okay. Any model you create will be an approximation of your data, so you should only include data that you care about approximating. If you don't want to approximate outliers, don't include them in your data.

In our data, we can clearly see a linear trend in most of the data. If that trend is what we care about approximating, it makes sense to exclude the outlier. 

Another reason to exclude outliers is that they sometimes represent errors. Our data doesn't measure anything, and so there's no clear way to tell if the outlier is an error or not. In the next chapter, we'll talk more about looking for outliers and errors in your data before building models.

## Summary

In this chapter, we went over the basic process of fitting a model. Here's the steps again:

* Determine which function family would approximate your data well.
* Choose a error metric. 
* Use an R function to find the specific function in your chosen function family that minimizes your chosen error metric. 



