
# Determine functional form

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
```

In Chapter 1, we mentioned three steps to building a model:

* Explore your data and choose an appropriate function family.
* Choose an error metric. 
* Use an R function to find the specific function within the chosen family that minimizes the error metric. 

This chapter focuses on the first step. We'll explore the relationship between diamond price and the 4 Cs and, by the end of the chapter, understand which function family or families might approximate the data well. 

## Families of linear functions

In this chapter, we'll restrict our attention to families of linear functions. In Chapter 1, we looked at the family of linear functions of a single variable. These functions have the form

`y = a_0 + a_1 * x_1`.

Recall that `a_0` is the y-intercept and `a_1` is the slope. We can also write this functional form as

`y = a_0 + f_x_1(x_1)` 

where `f_x_1(x_1) = a_1 * x_1`. This syntax will come in handy when we discuss discrete predictors later. To add more variables to the function, add more `f_x(x)` functions. The general form for linear functions of `n` variables is

`y = a_0 + f_x_1(x_1) + f_x_2(x_2) + ... + f_x_n(x_n)`

## Formulas

In Chapter 1, we mentioned that the modeling function `lm()` takes two arguments: a function family and data. `lm()`, likes many other modeling functions, takes the function family as a _formula_.  

Our model from Chapter 1 belonged to the family with the form `y = a_0 * a_1 * x_1`. As a formula, this functional form is `y ~ x_1`. You place the name of the response variable to the left of the `~` and the predictor variable(s) to the right of the `~`. To add more predictor variables, separate their names with `+`s:

`y ~ x_1 + x_2 + ... + x_n`

Formulas are their own class of R object, and you specify them unquoted, like the name of a variable. 

```{r}
y ~ x_1
class(y ~ x_1)
```

Because y-intercepts are so common in linear models, `lm()` includes one by default. If you don't want a y-intercept, add a `0` to your formula.

`y ~ 0 + x_1`

## Continuous predictors

Let's start with continuous predictors, as they should be familiar after reading Chapter 1. 

In Chapter 2, we modified `diamonds` quite a bit. Here are all of those modifications in one place:

```{r}
df <- 
  diamonds %>% 
  filter(x > 0, y > 0, z > 0) %>% 
  filter(y < 20, z < 10) %>% 
  filter(carat <= quantile(.$carat, probs = 0.99)) %>% 
  mutate(color = fct_rev(color))
```

Now, we can visualize the relationship between `carat` and `price` and look for a linear relationship. `geom_hex()` is a good choice, since there are tens of thousands of diamonds. 

```{r}
df %>% 
  ggplot(aes(carat, price)) +
  geom_hex() +
  scale_fill_viridis_c()
```

The larger diamonds cover a wide range of prices, so we'll add a smooth line to show the central tendency. 

```{r message=FALSE}
df %>% 
  ggplot(aes(carat, price)) +
  geom_hex() +
  geom_smooth() +
  scale_fill_viridis_c()
```

Notice how `price`, especially for the larger diamonds, varies substantially among diamonds of the same size. This variation suggests that `carat` is not the only variable that influences `price`. From the smooth line, we can also see that the relationship between `carat` and `price` is positive, but not linear. Luckily, we can still use a linear model by first transforming the data. We'll talk more about transformations in the next section. 

### Transformations

_Power law_ relationships take the form

`y = c * x^d`

where `c` and `d` are both constants. Taking the log of both sides gives us

`log(y) = log(c) + d * log(x)`

which might look familiar. Power laws are linear in terms of `log(y)` and `log(x)`, which is even clearer if we replace `log(c)` with `a_0` and `d` with `a_1`.

`log(y) = a_0 + a_1 * log(x)`

Power laws are common in modeling. You can determine if your data approximately follows a power law by taking the log of both the predictor and response and then visually checking if the relationship is linear.  We'll do this for `price` and `carat`.  

```{r message=FALSE}
df <-
  df %>% 
  mutate(
    log2_carat = log2(carat),
    log2_price = log2(price)
  ) 

df %>% 
  ggplot(aes(log2_carat, log2_price)) +
  geom_hex() +
  geom_smooth() +
  scale_fill_viridis_c()
```

The smooth line is now linear, suggesting that the functional form `log2(price) = a_0 + a_carat * log2(carat)` will approximate the relationship between `price` and `carat` well. 

To specify this function family in a formula, we have two options. We can refer directly to new variables created in `mutate()`:

`log2_price ~ log2_carat`

Or, we can apply the `log2()` transformations in the formula:

`log2(price) ~ log2(carat)`

Log-log transformations are common, but you can apply any transformation (to your response, predictor, or both) that makes your data linear.

You might wonder why you should bother transforming your data instead of just fitting a nonlinear model. Linear models are generally simpler, easier to interpret, and often produce better approximations.

### Visualizing

`carat` is the only continuous variable in the 4 Cs, but other data sets will have multiple possible continuous predictors. Here are two features to check for when considering adding a continuous predictor to your model. 

First, as you've already seen, the relationship between the predictor and the response should be linear. If necessary, transform the data so that the relationship is linear. If the relationship is nonlinear and you can't find a transformation that makes it linear, don't add the predictor to your linear model. 

Second, the predictor should influence the response. In our visualization of `price` and `carat`, we noticed that `price` increased as `carat` increased. If the predictor does not influence the response, you'll see something like this:

```{r echo=FALSE, cache=TRUE}
set.seed(234)

tibble(
  x = runif(n = 500, min = 0, max = 5),
  y = rnorm(n = 500)
) %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth(method = "loess")
```

The smooth line is horizontal, which tells you that `y` does not change with `x`. 

The `diamonds` data is relatively simple, and we've made it even simpler by restricting attention to the 4 Cs. When you have more variables, you'll need to conduct more preprocessing and pay more attention to which variables to include and which to exclude. If you'd like to learn more, Max Kuhn and Kjell Johnson's [Feature Engineering and Selection](https://bookdown.org/max/FES/intro-intro.html) is a good resource, particularly the section on [preprocessing](https://bookdown.org/max/FES/stroke-preprocessing.html).

## Discrete predictors

Earlier, we noticed that `price` varies substantially for diamonds of a given carat. In this section, we'll investigate if the other 3 Cs, `clarity`, `color`, and `cut`, account for this variation.

### Functional form

Discrete predictors work differently than continuous predictors. When a linear model includes a discrete predictor, a different constant will be added for each value of the variable.

We'll visualize the effect of `color` by adding a smooth line for each value to our our plot of `log2_price` and `log2_carat`. We left off the individual points to focus on the trends.^[Note that we used `method = "loess"` instead of the default `method = "gam"`. "loess" can take more time and memory for large amounts of data, but often gives better results. For more information, see https://dcl-data-vis.stanford.edu/continuous-continuous.html#smoothing]. 

```{r, cache=TRUE}
df %>% 
  ggplot(aes(log2_carat, log2_price, color = color)) +
  geom_smooth(method = "loess") +
  scale_color_discrete() +
  guides(color = guide_legend(reverse = TRUE))
```

The smooth lines look linear, and all appear to have similar slopes but different intercepts, which increase as `color` quality increases. This suggests that the following functional form, in which the intercept of the line depends on `color`, would approximate our data well. 

`log2(price) = a_0 + f_carat(log2(carat)) + f_color(color)`

In formula syntax, this functional form is

`log2(price) ~ log2(carat) + color`

We need `f_color()` to return a different constant for each value of `color`. We can write `f_color()` in R using `case_when()`.

```{r}
f_color <- function(color) {
  case_when(
    color == "D" ~ a_color_D,
    color == "E" ~ a_color_E,
    color == "F" ~ a_color_F,
    color == "G" ~ a_color_G,
    color == "H" ~ a_color_H,
    color == "I" ~ a_color_I,
    color == "J" ~ 0,
  )
}
```

The constant for J (the worst) diamonds is 0 because `a_0` represents the intercept for the J line. 

In the next section, we'll visualize `clarity` and `cut` and discuss visualizing discrete predictors in more detail. 

### Visualizing 

Now that you understand the form of `f_color()`, we can be more precise about what to look for when visualizing a discrete predictor. Here's a brief checklist:

* The smooth lines should all be approximately linear.

  Adding a discrete variable to the model will create a line for each value. These lines won't approximate the
  data well if the underlying relationships are not linear. 

* The smooth lines should all be approximately parallel to each other.

  In our example, the slopes of the lines depend only on our continuous predictor: `log2(carat)`. Changing `color`
  does not change the slope. Non-parallel smooth lines suggest that slope depends on both the continuous
  predictor and the discrete predictor. This is called an _interaction_. Interactions create nonlinear functions,
  so we won't discuss them further in this chapter. 
  
* The smooth lines should have different y-intercepts.

  Smooth lines that are stacked on top of each other and don't have different y-intercepts suggest that the discrete
  variable doesn't influence the response. 
  
`color` met all three of our criteria pretty well, although the smooth lines for the 4 best colors (D, E, F, and G) are almost entirely on top of each other. 

```{r, cache=TRUE}
df %>% 
  ggplot(aes(log2_carat, log2_price, color = color)) +
  geom_smooth(method = "loess") +
  scale_color_discrete() +
  guides(color = guide_legend(reverse = TRUE))
```

This overlap suggests that increasing quality above G does not influence price very much. We could try combining D, E, F, and G into one level. 

```{r, cache=TRUE}
df <-
  df %>% 
  mutate(color_combined = fct_collapse(color, DEFG = c("D", "E", "F", "G")))

df %>% 
  ggplot(aes(log2_carat, log2_price, color = color_combined)) +
  geom_smooth(method = "loess") +
  scale_color_discrete() +
  guides(color = guide_legend(reverse = TRUE))
```

The smooth lines are now more clearly separated. Later, when we fit our models, we'll compare a model that uses `color` with all 7 values to one that uses `color_combined`. 

Next, let's look at `clarity`.

```{r, cache=TRUE}
df %>% 
  ggplot(aes(log2_carat, log2_price, color = clarity)) +
  geom_smooth(method = "loess") +
  scale_color_discrete() +
  guides(color = guide_legend(reverse = TRUE))
```

`clarity` also mostly meets our criteria. The slope of the line for I1 looks a bit different than the rest. Let's check how many I1 diamonds there are.

```{r}
df %>% 
  count(clarity, sort = TRUE)
```

I1 diamonds are relatively rare, so it will be okay if our linear model doesn't approximate them as well as other diamonds. 

Also, notice how the `clarity` smooth lines are further apart than those for `color`. This separation suggests that `clarity` has more influence over `price` than `color`.

Finally, let's look at `cut`.

```{r, cache=TRUE}
df %>% 
  ggplot(aes(log2_carat, log2_price, color = cut)) +
  geom_smooth(method = "loess") +
  scale_color_discrete() +
  guides(color = guide_legend(reverse = TRUE))
```

`cut` also meets the criteria. Premium and Very Good are close together, so we might consider combining them into a single level. The smooth lines for `cut` are even closer together than those of `clarity`, so `cut` likely has the least effect on `price` of the 3 discrete Cs. 

## Summary

From our EDA, we now know that:

* We need to log-transform both `price` and `carat` to make the relationship linear. 
* `clarity`, `color`, and `cut` all influence `price`, and we can add any of them to our model. 
* `clarity` is the most important of the 3 Cs, followed by `color`, then `cut`.
* Increasing the quality of the color beyond G doesn't influence price very much. We could combine the top levels of `color` into one level before fitting our model. 

Even though we found that `carat`, `clarity`, `color`, and `cut` all influence price, we might not want to include them all into our model. In the next chapter, we'll talk about the downsides of adding too many predictors, and discuss how to choose an appropriate number. 




