
# Determine functional form

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
```

In Chapter 1, we mentioned three steps to building a model:

* Explore your data and choose an appropriate function family.
* Choose an error metric. 
* Use an R function to find the specific function in the function family that minimizes the error metric. 

In this chapter, we'll focus on step 1, exploring the relationship between each of the 4 Cs and diamond price. By the end of the chapter, we'll understand which function family or families could approximate our data well.

## Families of linear functions

In this chapter, we'll restrict our attention to families of linear functions. In chapter 1, we used looked at the family of linear functions of a single variable. These functions have the form

`y = a_0 + a_1 * x_1`.

Recall that `a_0` is the y-intercept and `a_1` is the slope of the line. We can also write the functional form as

`y = a_0 + f_x_1(x_1)` 

where `f_x_1(x_1) = a_1 * x_1`. This syntax will come in handy later. Our Chapter 1 data had only two variables: `y`, the response, and `x_1`, the sole predictor. `diamonds` has more potential predictors. To add more variables to the function, we just add more of the `f_x(x)` functions. Here's the general form for linear functions of `n` variables

`y = a_0 + f_x_1(x_1) + f_x_2(x_2) + ... + f_x_n(x_n)`

When you determine which specific family of linear functions to use to approximate your data, you choose how many different predictors to include in your model. For example, you might choose between a linear function of two variables

`y = a_0 + f_x_1(x_1) + f_x_2(x_2)`

or one of three

`y = a_0 + f_x_1(x_1) + f_x_2(x_2) + f_x_3(x_3)`

In this chapter, we'll explore the relationship between `price` and the 4 Cs and determine which of the these variables to include in our model. 

## Continuous predictors

Let's start with continuous predictors since these will be more familiar after reading Chapter 1. First, here's all the modifications we did to `diamonds` in the last chapter in one chunk:

```{r}
df <- 
  diamonds %>% 
  filter(x > 0, y > 0, z > 0) %>% 
  filter(y < 20, z < 10) %>% 
  filter(carat <= quantile(.$carat, probs = 0.99)) %>% 
  mutate(color = fct_rev(color))
```

Now, we can visualize the relationship between `carat` and `price` and look for a linear relationship. There are tens of thousands of diamonds, so we'll use `geom_hex()`.

```{r}
df %>% 
  ggplot(aes(carat, price)) +
  geom_hex() +
  scale_fill_viridis_c()
```

The larger diamonds cover a wide range of prices, so we'll add a smooth line to show the central tendency. 

```{r message=FALSE}
df %>% 
  ggplot(aes(carat, price)) +
  geom_hex() +
  geom_smooth() +
  scale_fill_viridis_c()
```

From this visualizations, we can make several observations relevant to the modeling process. First, `price` varies substantially for diamonds of a given carat, especially for the high-carat diamonds. This variation suggests that `price` is not just a function of `carat`. 
Second, although the relationship between `carat` and `price` is positive (larger diamonds cost more), the relationship doesn't look linear. Luckily, this doesn't mean we can't use a linear model. In the next section, we'll discuss how to apply transformations to your data to linear relationships. 

### Transformations

_Power law relationships_ are a common nonlinear relationship, and are linear if log-transform `y` and `x`. 

`log2(y) = a_0 + a_1 * log2(x)`

We can check if the relationship between `price` and `carat` follows a power law by taking the log of each variable and then re-running our `geom_hex()` visualization. 

```{r message=FALSE}
df <-
  df %>% 
  mutate(
    log2_carat = log2(carat),
    log2_price = log2(price)
  ) 

df %>% 
  ggplot(aes(log2_carat, log2_price)) +
  geom_hex() +
  geom_smooth() +
  scale_fill_viridis_c()
```

The smooth line is linear, which suggests that the functional form `log2(price) = a_0 + a_1 * log2(carat)` will approximate the relationship between `price` and `carat` well. We can also express the function as

`log2(price) = a_0 + f_carat(log2(carat))`

where `f_carat(log2(carat)) = a_1 * log2(carat)`. 

We can apply any transformation to the response and/or predictor. Let's call these transformation functions `g_*()`. Then, for a linear function of a single variable, we have

`g_y(y) = a_0 + f_x_1(g_x_1(x_1))`

Your task as a modeler is to find `g_y()` and `g_x_1()` such that the relationship between `g_y(y)` and `f_x_1(x_1)` is linear. 

Why do we want to transform the relationship into a linear one instead of just building a non-linear modeling? Linear models are simple, easily interpretable, and make use of the extensive algorithms and R functions developed around them. Linear models on transformed data often better approximate non-linear data than non-linearm models on un-transformed data. So, when you encounter a non-linear relationship, try to transform your response and/or predictor so that the relationship is linear.

Now that we've figured out the functional form of the relationship between `price` and `carat`, we can turn to the other 3 Cs: clarity, color, and cut. 

Because we log-transformed `price` and `carat`, the interpretation of `a_0` as the intercept and `a_1` as the slope is no longer correct. If we convert back to linear space, our function is

`price = 2^a_0 * carat^a_1`

Now, `2^a_0` is estimated price of a 1 carat diamond, while `a_1` is the amount that `price` increases every time `carat` doubles.

### Checklist

what to look for when considering whether or not to add a continuous predictor to your model. Here's a brief checklist:

* The relationship between the predictor (or the transformed predictor) and the response (or the transformed response) appears linear. 
* The response changes as the predictor changes. If you see something like this:

```{r echo=FALSE}
tibble(
  x = runif(n = 500, min = 0, max = 5),
  y = rnorm(n = 500)
) %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth(method = "loess")
```

`x` is not a good predictor of `y`.

* The predictor is not significantly correlated with any other continuous predictor in your model. Linear models can't handle predictors that are significantly correlated with each other. 

For example, say we were considering adding log-transformed `x` (diamond length) to our model.

```{r method=FALSE}
df %>% 
  ggplot(aes(log2(x), log2_price)) +
  geom_hex() +
  geom_smooth() +
  scale_fill_viridis_c()
```

The relationship appears linear. However, `log2(x)` and `log2(carat)` are highly correlated.

```{r}
df %>% 
  ggplot(aes(log2(x), log2(carat))) +
  geom_hex() +
  scale_fill_viridis_c()
```

## Discrete predictors

Earlier, we noticed that `price` varies substantially for diamonds of a given carat, suggesting that there are other variables responsible for price. In this section, we'll investigate if `clarity`, `color`, and/or `cut` account for this variation. 

### f(x) form

Let's focus on `color` first. If we add `color` to our linear model, we'd have the following functional form

`log2(price) = a_0 + f_carat(log2(carat)) + f_color(color)`

Because `color` is a discrete variable, `f_color()` can't be `a_1 * color`. You can't multiply D or E a number. Instead, when you include a discrete variable in a linear model, you create different lines, each with a different intercept, for each value of the variable. So, if we include `color` in our model, we'd create 7 different lines, one for each value of `color`. Each would have the same slope, but a different intercept. 

With that in mind, we need to figure out if we'd even want to do that for `color`. We want to understand if `color` accounts for any of the variation in `price` for a given value of `carat`. We'll visualize this by adding `color` to our plot of `log2_price` and `carat`. Before, we used a single smooth line to see the central tendency of the rleationship. Now, we'll create one smooth line for each value of `color`, leaving off the individual points to focus on the trends.^[Note we used `method = "loess"` here instead of the default `method = "gam"`. "loess" can take more time and memory for lots of data, but often gives better results. For more information, see https://dcl-data-vis.stanford.edu/continuous-continuous.html#smoothing.]. 

```{r}
df %>% 
  ggplot(aes(log2_carat, log2_price, color = color)) +
  geom_smooth(method = "loess") +
  scale_color_discrete() +
  guides(color = guide_legend(reverse = TRUE))
```

Here's some salient features to recognize from this plot:

* The lines are separated from each other, and the y-intercept of the line looks to increase as color increases in quality.
* Each smooth line is very close to linear.
* The lines are, for the most part, parallel to each other.

Imagine each smooth line as its own function. The slopes of these lines should be pretty similar, but the y-intercepts will be different. This is exactly what we want to represent in our linear function. When we add `f_color()`, we want our function to create a different line, with a different intercept, for each value of `color`. 

This plot is a great example of what it looks like when you should add a discrete predictor to your linear model. This means that `f_color()` should add a different value to `a_0` for different values of `color. In other words, we need a parameter (an `a_*`) for each value of `color`. 

Here's how we might write `f_color()` as an R function:

```{r}
f_color <- function(color) {
  case_when(
    color == "D" ~ a_color_D,
    color == "E" ~ a_color_E,
    color == "F" ~ a_color_F,
    color == "G" ~ a_color_G,
    color == "H" ~ a_color_H,
    color == "I" ~ a_color_I,
    color == "J" ~ a_color_J,
  )
}
```

Let's say we have the following diamond, selected from `df`.

```{r}
df %>% 
  slice(5)
```

To get its predicted price, we would plug its `carat` and `color` to 

`log2(price) = a_0 + f_carat(log2(carat)) + f_color(color)`

which gives us

`log2(price) = a_0 + a_1 * log2(df %>% slice(5) %>% pull(carat)) + a_color_J`

Now, all we'd need to do is figure out our `a_0` and `a_1`, which we'll cover in more detail in the next chapter. 

We actually only need `r nlevels(df$color) - 1` parameters for `f_color()`, because the intercept for the lowest quality color, D, will just be `a_0`. 

Now, let's interpret the `color` parameters. In log-log space, each `a_0 + a_color_*` is the y-intercept. In untrasformed space, we have the function

`price = 2^a_0 * 2^f_color(color) * carat^a_1`

So, for 1 carat diamonds where `carat^a_1` is 1:

`price = 2^a_0 * 2^f_color(color)`

`2^f_color` is a multiplier, giving you the multiplier for a diamond of that color. So, for `J` diamonds, `2^a_color_J` is the multiplier for J diamonds. For diamonds of a given carat, you get a `2^a_color_*` multiplicative effect above the base diamond color. 

In the next section, we'll visualize `clarity` and `cut` and go into more detail about what to look for when deciding whether or not to add a discrete predictor into your model. 

### Checklist 

Now that you understand the form of `f(x)` for discrete predictors, we can be clearer about what to look for in a visualization of a discrete predictor. Here's a brief checklist:

* Smooth lines should all be approximately linear
  If the smooth lines are not linear, then creating a line for each value of the discrete variable won't approximate the data well.

* Smooth lines should all be aproximately parallel to each other

  If the smooth lines aren't parallel, then each slope depends on both the continuous predictor and the discrete predictor. This is called an _interaction_ and, while we won't discuss interactions in more detail here, it's useful to know about them.
  
* Smooth lines should appear to have different y-intercepts 
  Finally, smooth lines are stacked on top of each, so that they do not appear to have different y-intercepts, indicate that the discrete variable does not influence the response. 

`color` met all three of our criteria pretty well, although take a look at the highest quality colors again.

```{r}
df %>% 
  ggplot(aes(log2_carat, log2_price, color = color)) +
  geom_smooth(method = "loess") +
  scale_color_discrete() +
  guides(color = guide_legend(reverse = TRUE))
```

D, E, F, and G are very close together, suggesting that increasing quality above G does not influence price very much. We could combine these three levels into one. 

```{r}
df <-
  df %>% 
  mutate(color_combined = fct_collapse(color, DEFG = c("D", "E", "F", "G")))

df %>% 
  ggplot(aes(log2_carat, log2_price, color = color_combined)) +
  geom_smooth(method = "loess") +
  scale_color_discrete() +
  guides(color = guide_legend(reverse = TRUE))
```

If we combined D, E, F, and G, the smooth lines are more clearly separated.

Now, let's look at `clarity`.

```{r}
df %>% 
  ggplot(aes(log2_carat, log2_price, color = clarity)) +
  geom_smooth(method = "loess") +
  scale_color_discrete() +
  guides(color = guide_legend(reverse = TRUE))
```

`clarity` also mostly meets our criteria. The slope of the line for I1 looks a bit different than the rest. Let's check how many I1 diamonds there are.

```{r}
df %>% 
  count(clarity, sort = TRUE)
```

I1 diamonds are relatively rare, so we won't worry about our linear model not approximately them as well as the others. Next, let's look at `cut`.

```{r}
df %>% 
  ggplot(aes(log2_carat, log2_price, color = cut)) +
  geom_smooth(method = "loess") +
  scale_color_discrete() +
  guides(color = guide_legend(reverse = TRUE))
```

`color` also meets the criteria. Premium and Very Good are close together, so we might consider combining them into a single level like we did with `clarity`. 

