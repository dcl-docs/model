
# Fitting basics

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
```


In [Chapter 1](http://dcl-model.stanford.edu/model_basics.html#summary), we identified three steps involved in basic modeling:

* Explore your data and choose an appropriate function family.
* Choose an error metric.
* Use an R function to find the specific function within the chosen family that minimizes the error metric.

In the previous chapter, we focused on the first step, visualizing the diamonds data to understand the functional form of the relationship between `price` and `carat`. Now, we'll focus on the second two steps. We'll refer to the third step---using an R function to identify a specific function---as _fitting_ a model. 

In this chapter, we're going to fit a model on all of the diamonds data at once to retain a focus on the basics of fitting and evaluating a model. However, you generally won't want to fit your model on all of your data. If you use all your data to train your model, you won't be able to understand how your model will do on new, previously unseen data. We'll address this issue in the next chapter. 

## Tidymodeling

Tidymodeling is a new approach to modeling in R, and is still under development. The tidymodels world, like the tidyverse, includes an entire ecosystem of packages. You can load a subset of the tidymodels packages with the tidymodels meta-package. 

```{r message=FALSE, warning=FALSE}
library(tidymodels)
```

In this chapter, we'll also use a couple of functions from a slightly older package, modelr.

```{r message=FALSE, warning=FALSE}
library(modelr)
```

Tidymodels is a vast and flexible world. Here, we've tried to focus our explanation on the most basic and widely applicable functions. You should also be aware that tidymodels is still evolving, and will probably change in the future.  

## Fit

### Assemble data

We've made many modifications to `diamonds` over the past two chapters, including removing diamonds with impossible dimensions and filtering out very large diamonds. The following code repeats these manipulations and stores the result in a new tibble, `df`. 

```{r}
df <- 
  diamonds %>% 
  filter(x > 0, y > 0, z > 0) %>% 
  filter(y < 20, z < 10) %>% 
  filter(carat <= quantile(.$carat, probs = 0.99)) %>% 
  mutate(
    color = fct_rev(color),
    color_combined = fct_collapse(color, "DEFG" = c("D", "E", "F", "G"))
  )
```

In the previous chapter, we also found a linear relationship between the log of `carat` and the log of `price`. To make it easier to fit our model, we'll log-transform both `price` and `carat`, creating two new variables. 

```{r}
df <-
  df %>% 
  mutate(
    log2_price = log2(price),
    log2_carat = log2(carat)
  )
```

Going forward, we'll use `df` instead of `diamonds`.

### Specify model

In the tidymodels ecosystem, the first step in model fitting involves specifying a type of model. There are many types of models, but this book focuses on just one: linear regression. A tidymodels model specification has two features:

* A model type.
* An engine.

The model type describes the general modeling approach, like linear regression, logistic regression, or decision tree. Each model type supported by tidymodels has its own function. You can see a list of all supported model types [on the parsnip website](https://tidymodels.github.io/parsnip/articles/articles/Models.html). parsnip is the tidymodels package responsible for creating models.^[The package is called "parsnip" as a play on the name of its predecessor, caret. Parsnips have no special relevance to modeling. `r emo::ji("carrot")`]

Here, we want linear regression, so we'll use the function `linear_reg()` to specify the model type.

```{r}
linear_reg()
```

The engine specifies the algorithm to use to fit the model. There are many ways of carrying out linear regression, but the most common involves the R function `lm()`. 

The function `set_engine()` takes a string indicating the algorithm. We'll use `engine = "lm"` to start.

```{r}
lm_spec <-
  linear_reg() %>% 
  set_engine("lm")
```

Model-fitting in the tidymodels world requires creating many variables, and it's helpful to develop a common naming scheme. We'll give all model specification variables the suffix `_spec`.

The [same parsnip page](https://tidymodels.github.io/parsnip/articles/articles/Models.html) lists all possible engines for every model type.

Tidymodels makes it easy to change the algorithm used to fit the model. For example, we can switch to a Bayesian model by trading `"lm"` for `"stan"`.

```{r}
stan_spec <-
  linear_reg() %>% 
  set_engine(engine = "stan")
```

### Fit

We've specified a model type, selected the method of fitting that model, and stored the result in a variable. 

```{r}
lm_spec
```

We still need to specify a function family with a formula. In the previous chapter, we identified multiple possible function families. For now, we'll use the simplest:

`log2_price ~ log2_carat`

This formula specifies a function of the form:

`log2_price = a_0 + a_1 * log2_carat`

The `fit()` function takes a model specification, formula, and data. 

```{r}
lm_fit <- 
  fit(
    lm_spec,
    log2_price ~ log2_carat,
    data = df
  )

lm_fit
```

The output printed under `Coefficients:` lists the values of `a_0` and `a_1` that minimized our error metric (RMSE, because we used `engine = "lm"`) for our function family. We'll show you how to extract these soon. 

First, let's take a closer look at `lm_fit`. `lm_fit` is a list.

```{r}
typeof(lm_fit)
names(lm_fit)
```

The `fit` element is the most useful. It contains the fitted coefficients, predicted values, residuals, and other information. 

```{r}
names(lm_fit$fit)
```

You can call `summary()` on `lm_fit$fit` to see an overview of the model.

```{r}
summary(lm_fit$fit)
```

The coefficients are stored in the `coefficients` element of `lm_fit$fit`.

```{r}
lm_fit$fit$coefficients
```

You can also use the function `coef()` to extract the coefficients from `lm_fit$fit`.

```{r}
coef(lm_fit$fit)
```

```{r include=FALSE}
a_0 <- round(coef(lm_fit$fit)[["(Intercept)"]], 2)
a_1 <- round(coef(lm_fit$fit)[["log2_carat"]], 2)
```

The coefficient called `(Intercept)` is our `a_0`, and the coefficient called `log2_carat` is our `a_1`. You can extract individual coefficients by name.

```{r}
coef(lm_fit$fit)[["(Intercept)"]]
```

Our fitted model is therefore

`log2_price = `r a_0` + `r a_1` * log2_carat`

We'll talk more about how to interpret this once we visualize our model's predictions. 

## Evaluate

Our model approximates the relationship between `log2_price` and `log2_carat`. To evaluate and understand how our model approximates this relationship, we're going to:

* Visualize the predictions against the actual data.
* Visualize the residuals.
* Calculate error metrics. 

First, we'll need a tibble with:

* The actual `log2_carat` and `log2_price` values.
* Our model's predicted `log2_price` values.
* The residual for each point (the difference between the actual and predicted `log2_price`).

`df` already contains the actual `log2_carat` and `log2_price` values. We'll add columns for the predicted values and the residuals. 

There are multiple ways to add predictions and residuals to a tibble. Our recommended approach uses `modelr::add_predictions()` and `modelr::add_residuals()`. Note that you pass each function `lm_fit$fit`, not `lm_fit`.

```{r}
df_predictions <-
  df %>% 
  add_predictions(lm_fit$fit) %>% 
  add_residuals(lm_fit$fit)

df_predictions %>% 
  select(log2_carat, log2_price, pred, resid)
```

`add_predictions()` adds a `pred` column and `add_residuals()` adds `resid` column. Each value in `pred` is the predicted value of `log2_price` for the corresponding value of `log2_carat`. Each residual is equal to 

`log2_carat` - `pred`

### Visualize predictions

Now, we're ready to visualize our model. First, we'll plot our model's predictions against the actual data. 

```{r eval=FALSE}
df_predictions %>% 
  ggplot(aes(log2_carat)) +
  geom_hex(aes(y = log2_price), alpha = 0.85) +
  geom_line(aes(y = pred), color = "blue", size = 1) +
  scale_fill_viridis_c() 
```
```{r echo=FALSE}
df_predictions %>% 
  ggplot(aes(log2_carat)) +
  geom_hex(aes(y = log2_price), alpha = 0.85) +
  geom_line(aes(y = pred), color = "blue", size = 1) +
  annotate_model(x = 0.75, y = 12.8, model = lm_fit$fit) +
  scale_fill_viridis_c() 
```

The model appears to approximate the data relatively well. The line travels through the hotspots and isn't swayed by outliers. As we noted in the previous chapter, there's a lot of variation in price for diamonds of a given carat, particularly for larger diamonds. Our model can't account for this variation because we only used one predictor: `log2_carat`. Later, we'll incorporate some of the categorical variables, and explore how those models might approximate this variation. 

`a_0`, the y-intercept, represents the predicted value of `log2_price` when `log2_carat` is 0.

```{r echo=FALSE}
df_predictions %>% 
  ggplot(aes(log2_carat)) +
  geom_hex(aes(y = log2_price), alpha = 0.85) +
  geom_vline(xintercept = 0, color = "white", size = 1) + 
  geom_line(aes(y = pred), color = "blue", size = 1) +
  geom_point(x = 0, y = a_0, color = "red") +
  annotate(
    "label",
    x = 0.05,
    y = 11.8,
    hjust = 0,
    label = str_glue("log2_carat = 0\nlog2_price = {round(a_0, 3)}")
  ) +
  scale_fill_viridis_c() 
```

`a_1` is the slope the line, the amount that `log2_price` each time `log2_carat` increases by 1.

```{r include=FALSE}
df_predictions %>% 
  ggplot(aes(log2_carat)) +
  geom_hex(aes(y = log2_price), alpha = 0.85) +
  geom_segment(x = 0, xend = 0, y = a_0, yend = a_0 + a_1 * 1, color = "white", size = 1) +
  geom_segment(x = 0, xend = 1, y = a_0 + a_1 * 1, yend = a_0 + a_1 * 1, color = "white", size = 1) +
  geom_line(aes(y = pred), color = "blue", size = 1) +
  annotate(
    "label",
    x = -0.04,
    y = 13,
    hjust = 1,
    label = str_glue("a_1 = {round(a_1, 3)}")
  ) +
  scale_fill_viridis_c() 
```

We're still in log-log space, but we don't usually talk about diamonds in log-log space. To make our model more interpretable, we'll translate our model back into linear space. Here's our model again:

`log2_price = `r a_0` + `r a_1` * log2_carat`

solving for `price` gives us

`price = 2^`r a_0` * carat^`r a_1``

Here's how to interpret the model:

```{r include=FALSE}
model <- function(carat) {
  2^coef(lm_fit$fit)[["(Intercept)"]] * carat^coef(lm_fit$fit)[["log2_carat"]]
}

price_diff_small <- model(0.5) - model(0.4)
price_diff_large <- model(2.1) - model(2.0)
```
* `2^`r a_0`` is the predicted price when `carat` is 1, so our model predicts that 1 carat diamonds are `r scales::dollar(2^a_0, accuracy = 1)`.
* `a_1` (`r a_1`) is the exponent on `carat`. Each time `carat` increases by 1%, `price` increases by `r a_1` percent. This means that diamond price increases more steeply for larger diamonds than for smaller diamonds. Buying a 0.5 carat instead of a 0.4 carat diamond will only cost you `r scales::dollar(price_diff_small, accuracy = 1)` more, but buying a 2.1 carat instead of a 2.0 carat will cost you `r scales::dollar(price_diff_large, accuracy = 1)` more.

Here's what the model looks like against the data in linear space.

```{r}
df_predictions %>% 
  ggplot(aes(carat)) +
  geom_hex(aes(y = price), alpha = 0.85) +
  geom_line(aes(y = 2^pred), color = "blue", size = 1) +
  scale_fill_viridis_c() 
```

### Visualize residuals

Next, we'll visualize the residuals. Ideally, all residuals will be very small. For our data, however, we know that price varies widely for every carat, so some residuals are likely to be large. 

We also want to check that our residuals don't vary with `carat`. If they do, it suggests our model fundamentally misunderstands the relationship between `price` and `carat`, or that we're missing important predictors. 

`df_predictions` already has our residuals in the `resid` column. We'll plot the residuals on the y-axis and `carat` on the x. Again, the formula for the residuals is

`log2_carat - pred`

so a residual greater than one means our model under-estimated the price of the diamond, while a residual less than one means our model over-estimated the price of the diamond.

We plotted `resid` against `carat` because `carat` is more easily interpretable, but `log2_carat` would also work.

```{r eval=FALSE}
df_predictions %>% 
  ggplot(aes(carat, resid)) +
  geom_hex() +
  geom_hline(yintercept = 0, color = "white", size = 1) +
  geom_smooth() + 
  scale_fill_viridis_c()
```

```{r echo=FALSE, message=FALSE}
df_predictions %>% 
  ggplot(aes(carat, resid)) +
  geom_hex() +
  geom_hline(yintercept = 0, color = "white", size = 1) +
  geom_smooth() + 
  annotate(
    geom = "text",
    x = 1.6,
    y = 1.5,
    label = "Diamonds are more\nexpensive than predicted",
    hjust = 0
  ) +
  annotate(
    geom = "text",
    x = 0.25,
    y = -1.8,
    label = "Diamonds are less expensive\nthan predicted",
    hjust = 0
  ) +
  scale_fill_viridis_c()
```

Because there are so many diamonds, we've used `geom_hex()` and a smooth line. If you have less data, you may want to use `geom_point()` and omit the smooth line. A reference line at 0 is always a good idea.

The residuals are, for the most part, centered around 0, which is a good sign for our model. If the model systematically under- or over-estimated price, we would see more residuals either above or below the 0 line.

There also isn't much of a trend between `carat` and `resid`, another good sign for our model. You can see that the model tends to over-estimate the price of diamonds larger than ~1.75 carats.

The residuals vary substantially for 0.5 and 1.0 carat diamonds, which makes sense given that the prices for these diamonds vary widely.

### Error metrics

Error metrics also provide information about a model's performance. Error metrics will be more important when you start splitting your data in training and testing sets, but we'll show you how calculate them here as an introduction.

The tidymodels package [yardstick](https://tidymodels.github.io/yardstick/) contains many useful error metric functions. You can see a full list on the [yardstick website](https://tidymodels.github.io/yardstick/reference/index.html).

Many of the error metric functions take three arguments: `data`, `truth`, and `estimate`. You pass your data to `data`, the name of the column with the true values to `truth`, and the name of the column with the predicted values to `estimate`. We'll use the `rmse()` function to calculate RMSE.

Note that modelr also contains an `rmse()` function. To make sure you're using the right version, use `yardstick::rmse()`.

```{r}
yardstick::rmse(
  data = df_predictions,
  truth = log2_price,
  estimate = pred
)
```

## Code summary

For reference, we've assembled all the code in one place for you.

```{r}
# Specify model
lm_spec <-
  linear_reg() %>% 
  set_engine("lm")

# Fit model
lm_fit <-
  fit(
    lm_spec,
    log2_price ~ log2_carat,
    data = df
  )

# Add predictions and residuals
df_pred <-
  df %>% 
  modelr::add_predictions(model = lm_fit$fit) %>% 
  modelr::add_residuals(model = lm_fit$fit)

# Calculate error
lm_error <-
  df_pred %>% 
  yardstick::rmse(truth = log2_price, estimate = pred)
```



