[["index.html", "Data Modeling Welcome How to read this book An evolving book", " Data Modeling Sara Altman, Bill Behrman 2022-04-16 Welcome How to read this book You can easily copy the code in this book by hovering over a chunk and then clicking the copy button that appears in the upper-right corner. Every dataset we use is contained in an R package, making it easy to follow along and replicate our examples. Many of the datasets are in our own dcldata package. To install dcldata, copy the following code and paste it into RStudio. # You can copy this code with the copy button! # First, install remotes if you don&#39;t already have it: # install.packages(&quot;remotes&quot;) remotes::install_github(&quot;dcl-docs/dcldata&quot;) To access the datasets, load the relevant package library(dcldata) and then call the dataset by name. anscombe_1 #&gt; x_1 y #&gt; 1 10 8.04 #&gt; 2 8 6.95 #&gt; 3 13 7.64 #&gt; 4 9 8.81 #&gt; 5 11 8.33 #&gt; 6 14 9.90 #&gt; 7 6 7.24 #&gt; 8 4 4.25 #&gt; 9 12 10.84 #&gt; 10 7 4.82 #&gt; 11 5 5.68 All datasets have accompanying documentation, which you can find with the ? operator. ?anscombe_1 You can also view the names of all datasets in a given package with the data() function. data(package = &quot;dcldata&quot;) An evolving book This book is not intended to be static. Starting in January 2020, we use this book to teach modeling in the Stanford Data Challenge Lab (DCL) course. The DCL functions as a testing ground for educational materials, as our students give us routine feedback on what they read and do in the course. We use this feedback to constantly improve our materials, including this book. The source for the book is also available on GitHub where we welcome suggestions for improvements. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["model_basics.html", "1 Model basics 1.1 What is a model? 1.2 Supervised learning 1.3 Choosing a function family 1.4 Classical modeling 1.5 Bayesian modeling 1.6 Summary", " 1 Model basics 1.1 What is a model? The world is complicated and messy, and there are endless details to even simple phenomena. To understand and navigate the world, we construct and use models. For example, think about traffic. You probably have a simple mental model that says traffic will be worse at rush hour than at two in the morning. You can use this model to describe how traffic varies throughout the day, but you can also use it to predict the level of traffic at a given hour. Your mental model of traffic, like any model, is an approximation. It tries to capture relevant information while ignoring noise and less important details. Your traffic mental model will never fully explain traffic, and so you’ll never be a perfect predictor of how many cars will be on the road at any given time. However, if it’s a good model, it will be useful. The type of models we’ll discuss in this book are similar to your mental model of traffic: they are approximations; you can use them to both describe and predict the world; and, while they will never be completely accurate, they can be useful. 1.2 Supervised learning You can divide up the world of models into two categories: supervised and unsupervised. In the following, we’ll discuss supervised learning, but it’s useful to understand the difference between the two. Supervised models are functions. They approximate the relationship of one variable to others. The variable you want to explain (e.g., traffic) is called the response and the variables (e.g, time of day) you use to explain the response are called predictors. Unsupervised models don’t have a response variable. Instead of trying to understand a relationship between predictors and a response, unsupervised models try to find patterns in data. When building a supervised model, your job as the modeler is to find a function that approximates your data. Functions map from one or more variables (e.g., time) to another (e.g., amount of traffic). You can use your approximation function to understand and describe your data, as well as make predictions about your response variable. There are an infinite number of types of functions, so how do you know where to look? The first step is to explore your data and determine which function family (or families) would best approximate your data. Function families are sets of functions with the same functional form. In the next section, we’ll talk more about the linear function family. Then, in Chapter 3, we’ll discuss how to use exploratory data analysis to choose a function family to approximate your data. 1.3 Choosing a function family Anscombe’s quartet is a set of four small data sets. In this section, we’ll use a slightly modified version of the first data set in the quartet. x_1 y 10 8.04 8 6.95 13 7.64 9 8.81 11 8.33 14 9.90 6 7.24 4 4.25 12 10.84 7 4.82 5 5.68 Recall that we said the first step to fitting a model is to determine which function family best approximates your data. In modeling, some of the most common and useful function families are linear. There are an infinite number of linear function families and, later, we’ll talk about how to decide exactly which family to use. For now, we’ll introduce the family of linear functions of a single variable. Functions in this family take the following form: y = a_0 + a_1 * x_1 . x_1 is your input variable, the variable that you supply to the function in hopes of approximating some other variable. In our traffic example, x_1 is the time of day. a_0 and a_1 are the parameters. These two numbers define the line. The only difference between functions in the family of linear functions are their values of a_0 and a_1. To visualize this, here’s a plot of many different lines, each of which has a different combination of a_0 and a_1. a_0 defines the y-intercept, the y-value your function will produce when x_1 is 0. a_1 is the slope, which tells you how much y changes for every one unit increase in x_1. These two parameters define a linear function, and so to fit a linear model, you just have to determine which a_0 and a_1 best approximates your data. As you’ll learn in Chapter 3, visualization is crucial to determining functional form. Let’s visualize the relationship between x_1 (the predictor) and y (the response). The relationship between x_1 and y looks linear, so a function in the linear family will likely make a good approximation. There are infinitely many functions you could use. Let’s pick one function and take a closer look. By glancing at the plot, you can tell that this function isn’t doing the best job of approximating our data. Most of the points where x_1 &lt; 9 fall above our line, and most of the points where x_1 &gt; 9 fall below the line. The distances between the line and each of the data points are called residuals (or errors). Each residual represents the actual value of y associated for a given x_1 and the y value that the model predicts for that x_1. The larger a residual, the worse your model approximates y at that value of x_1. We now turn to two methods for modeling your data, once you’ve chosen a function family with which to model. 1.4 Classical modeling Classical modeling finds the function in a function family for which the data would be most likely. For a linear function family, and with the assumption that the residuals are normally distributed with mean 0, this is equivalent to finding the function in the family that minimizes the sum of the squares of the residuals. In this case, the method is referred to as least squares regression. The R function lm() fits linear models using classical least squares regression. lm() needs two arguments: a function family and your data. In later chapters, you’ll learn how use lm(). For now, we’ll just show you the results for our example data. Here’s the model that lm() came up with: y = 3.0 + 0.5 * x_1 . And here’s the model plotted with the data: You can tell from the plot that this fitted model does a much better job of approximating our data than our previous attempt. Again, a_0 is the intercept, so we know that our model predicts that y will be 3.0 when x_1 is 0. a_1 is the slope, which means that our model predicts a 0.5 increase in y each time x_1 increases by 1. 1.5 Bayesian modeling In addition to providing the parameters for a function from a function family, the Bayesian formulation provides an estimate of the parameters’ probability distribution. This enables you to understand the uncertainties in the model and its predictions. Bayesian modeling also allows you to incorporate additional prior information into the modeling process. The stan_glm() function in the rstanarm package fits generalized linear models using Bayesian modeling. It needs two arguments: a function family and your data. In later chapters, you’ll learn how use stan_glm(). For now, we’ll just show you the results for our example data. Here’s the model that stan_glm() came up with: y = 3.0 + 0.5 * x_1 . And here’s the model plotted with the data: stan_glm() provides distributions for the parameters a_0 and a_1. The values above are the medians of these distributions. These values are very close to those returned by lm(). The joint distribution of the parameters and the standard deviation of the residuals allows us to express the uncertainties in the model and its predictions. Predictive uncertainties are shown in the grey ribbons in the plot. 1.6 Summary In this chapter, we went over the basics of a supervised model. Before you can fit a model, however, you first have to understand your data and check for any problems. The simple data used in this chapter doesn’t measure anything real, but you’ll typically be fitting models to complex, messy datasets. In the next chapter, we’ll discuss how to use exploratory data analysis to understand and prepare your data before starting the modeling process. "],["understand_your_data.html", "2 Understand your data 2.1 Understand your variables 2.2 Check for problems 2.3 1D EDA 2.4 Summary", " 2 Understand your data library(tidyverse) The diamonds dataset, in the ggplot2 package, contains information on 53,940 round diamonds from the Loose Diamonds Search Engine. Each row of diamonds represents the features of a single diamond, including a set of characteristics called the four Cs: carat, clarity, color, and cut. Over the next several chapters, we’ll model the relationship between the four Cs and diamond price, but before we dive into modeling, it’s important to understand our data. In this chapter, we’ll explore diamonds, visualize the important variables, look for errors and outliers, and create a dataset that is ready for modeling. 2.1 Understand your variables 2.1.1 Read the documentation When working with a new dataset, the first step is to read the documentation. diamonds is built into ggplot2 and you can find the documentation at ?diamonds. The Loose Diamonds Search Engine, the original source, also contains helpful information about the variables. As we said earlier, diamonds have a set of features called the four Cs: carat, clarity, color, and cut. diamonds contains these four features, as well as information on price and dimension. Here’s an overview of the different variables, obtained from the ?diamonds page and the original website. carat is a measure of diamond weight. One carat is equivalent to 0.2 grams. clarity refers to how clear a diamond is. Diamonds often contain imperfections like cracks or mineral deposits. The fewer and less noticeable a diamond’s imperfections, the better its clarity. clarity contains 8 ordered levels, from “I1” (the worst) to “IF” (the best). color refers to the color of the diamond. Colorless diamonds are considered better than diamonds with a yellow tint. diamonds contains diamonds of 7 different colors, represented by different letters. “D” - “F” diamonds are considered colorless, while “G” - “J” diamonds have a very faint color. cut refers to how a rough diamond is shaped into a finished diamond. Better cuts create more symmetrical and luminous diamonds. cut has 5 ordered levels: “Fair,” “Good,” “Very Good,” “Premium,” “Ideal.” x, y, z, depth, and table are various measures of a diamond’s size, in millimeters. 2.1.2 glimpse() glimpse() returns a useful snapshot of a dataset. glimpse(diamonds) #&gt; Rows: 53,940 #&gt; Columns: 10 #&gt; $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.… #&gt; $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver… #&gt; $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,… #&gt; $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, … #&gt; $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64… #&gt; $ table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58… #&gt; $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34… #&gt; $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.… #&gt; $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.… #&gt; $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.… glimpse() shows us the number of diamonds (53,940) and variables (10), and a peek at the values in each column. We can also see the different variables types: double (dbl), ordered (ord), and integer (int). We’ll talk more about variable type later on in this chapter. 2.1.3 summary() summary() is another helpful overview function. summary(diamonds) #&gt; carat cut color clarity depth #&gt; Min. :0.20 Fair : 1610 D: 6775 SI1 :13065 Min. :43.0 #&gt; 1st Qu.:0.40 Good : 4906 E: 9797 VS2 :12258 1st Qu.:61.0 #&gt; Median :0.70 Very Good:12082 F: 9542 SI2 : 9194 Median :61.8 #&gt; Mean :0.80 Premium :13791 G:11292 VS1 : 8171 Mean :61.7 #&gt; 3rd Qu.:1.04 Ideal :21551 H: 8304 VVS2 : 5066 3rd Qu.:62.5 #&gt; Max. :5.01 I: 5422 VVS1 : 3655 Max. :79.0 #&gt; J: 2808 (Other): 2531 #&gt; table price x y z #&gt; Min. :43.0 Min. : 326 Min. : 0.00 Min. : 0.0 Min. : 0.0 #&gt; 1st Qu.:56.0 1st Qu.: 950 1st Qu.: 4.71 1st Qu.: 4.7 1st Qu.: 2.9 #&gt; Median :57.0 Median : 2401 Median : 5.70 Median : 5.7 Median : 3.5 #&gt; Mean :57.5 Mean : 3933 Mean : 5.73 Mean : 5.7 Mean : 3.5 #&gt; 3rd Qu.:59.0 3rd Qu.: 5324 3rd Qu.: 6.54 3rd Qu.: 6.5 3rd Qu.: 4.0 #&gt; Max. :95.0 Max. :18823 Max. :10.74 Max. :58.9 Max. :31.8 #&gt; Notice that there are no NA’s in the data (for non-character variables, summary() displays the number of NAs in each column, if any exist). Take a look at the minimum and maximum values for each variable. carat and price both cover a large range of values. x, y, and z all have 0 as a minimum value, even though it’s impossible for a diamond to actually have a length, width, or depth of 0 mm. y and z also have very large maximums. We’ll investigate these values in the next sections. 2.2 Check for problems x, y, and z all contain some suspicious values. There are diamonds with 0s for these dimensions, which is impossible. There are also diamonds with large, improbable dimensions. A diamond with a y (width) of 58.9 mm, or 2.32 inches, would be similar in size to some of the largest diamonds in the world, and would surely cost more than $18,000. We should check how many diamonds have an x, y, or z of 0. diamonds %&gt;% filter(x == 0 | y == 0 | z == 0) %&gt;% nrow() #&gt; [1] 20 Luckily, there are only 20 (out of over 50,000), so we can just remove them. df &lt;- diamonds %&gt;% filter(x &gt; 0, y &gt; 0, z &gt; 0) Now, we can visualize the distributions to investigate the suspiciously high values. Boxplots are a good choice when you’re looking for outliers. df %&gt;% ggplot(aes(x, factor(1))) + geom_boxplot() + scale_y_discrete(breaks = NULL, labels = NULL, name = NULL) x has outliers, but they aren’t very extreme. A diamond with an 10mm (0.39 in) length seems plausible. How about y and z? df %&gt;% ggplot(aes(y, factor(1))) + geom_boxplot() + scale_y_discrete(breaks = NULL, labels = NULL, name = NULL) df %&gt;% ggplot(aes(z, factor(1))) + geom_boxplot() + scale_y_discrete(breaks = NULL, labels = NULL, name = NULL) The large values of y and z we saw in the summary() output are clearly extreme outliers. We’ll remove these 3 diamonds. df &lt;- df %&gt;% filter(y &lt; 20, z &lt; 10) We also noted that carat and price both cover a large range of values. We’ll take a closer look in the next section. 2.3 1D EDA Now that we’ve removed problematic data, we can explore each of our variables. One-dimensional exploratory data analysis or 1D EDA refers to the fact that, for now, we’ll just look at each variable by itself. In the next chapter, we’ll start exploring the relationships between variables. 2.3.1 Variable types First, it’s important to understand the difference between continuous and discrete variables. Continuous variables can take on an infinite number of possible values. carat, for example, is a continuous variable. A diamond can be 1.00 carats, 1.001 carats, 1.0001 carats, etc. R considers both integers and doubles to be continuous variables. Discrete variables can take on only a finite number of possible values. In diamonds, cut is a discrete variable, as a diamond can be in only one of 6 different cut groups. Characters, logicals, and factors are discrete variables. R also further divides factors into ordered and unordered factors. Recall from the glimpse() output that clarity, color, and cut all have type &lt;ord&gt;, which indicates an ordered factor. As we’ll explain later, many modeling algorithms treat ordered factors differently than unordered factors or character variables. 2.3.2 Continuous variables We’ll use histograms to understand our continuous variables’ distributions. First, let’s look at carat. df %&gt;% ggplot(aes(carat)) + geom_histogram(binwidth = 0.01) Earlier, we mentioned that carat has a large range. Our histogram makes it clear that the distribution has a very long tail. Most diamonds are under 2.5 carats, but the diamonds above 2.5 carats still account for over half of the range. Building a model requires sufficient data. We can expect to build a fairly accurate model for the diamonds under 2.5 carats, but there aren’t enough diamonds above 2.5 carats. These outlier, high-carat diamonds could distort our model, so we’ll restrict our attention to 99% of the diamonds and ignore the largest 1%. 99% of our diamonds are less than or equal to 2.18 carats. quantile(df$carat, probs = 0.99) #&gt; 99% #&gt; 2.18 We’ll filter df to exclude diamonds above the 99% quantile. df &lt;- df %&gt;% filter(carat &lt;= quantile(df$carat, probs = 0.99)) Now that we’ve focused the data, let’s visualize the distribution again. df %&gt;% ggplot(aes(carat)) + geom_histogram(binwidth = 0.01) Notice the spikiness of the distribution. It will be easier to tell where these spikes are if we add more x-axis breaks. df %&gt;% ggplot(aes(carat)) + geom_histogram(binwidth = 0.01) + scale_x_continuous(breaks = seq(0.2, 2.2, 0.1), minor_breaks = NULL) The spikes appear at or slightly above the even sizes of 0.3, 0.4, 0.5, 0.7, 0.9, 1.0, 1.2, 1.5, 1.7, and 2.0 carats. The diamonds in diamonds are cut from raw diamonds, so these spikes suggest that diamonds tend to be marketed at these sizes. Engagement rings, for example, tend to be around 1 carat, which could explain that spike. Now, let’s look at price. df %&gt;% ggplot(aes(price)) + geom_histogram(binwidth = 50) The price distribution also has a long tail, even though we’ve filtered out the largest diamonds. Interestingly, there’s a gap in the distribution around $1,500, probably caused by an error importing the data. With simple 1D EDA, we can already draw a conclusion for modeling this data. Notice how the price distribution doesn’t follow the same spiky pattern as the carat distribution, suggesting that price is not just a simple linear function of carat. We’ll come back to this idea in the next chapter. 2.3.3 Discrete variables Now, we can turn to the discrete Cs: clarity, color, and cut. For discrete variables, we’ll use geom_bar(). plot_bar &lt;- function(var, data = df) { data %&gt;% ggplot(aes({{ var }})) + geom_bar() } plot_bar(clarity) The best clarity classes are relatively rare, and most diamonds are in the SI1 category, the third worst. There are also very few diamonds with the worst clarity, I1. plot_bar(cut) Interestingly, cut is very different. Ideal cut diamonds are very common, and the number of diamonds in each category increases as quality increases. Maybe cut is a feature that diamond manufacturers have more control over, whereas clarity is more a feature of the raw diamond. The factor levels of clarity and cut are both in ascending order of quality, but color is in descending order. plot_bar(color) D is the best color and J is the worst. To transform color into ascending order, we can use fct_rev() to reverse the order of the factor levels. df &lt;- df %&gt;% mutate(color = fct_rev(color)) Now, color will plot in ascending order. plot_bar(color) The most common color is G, which is in the middle of the distribution. There are more diamonds in the top half of the distribution than in the bottom. 2.4 Summary Before constructing a model, take the time to explore and understand your data. It’s easier to build a good model if you understand what your variables actually measure, and it’s important to check for errors that need to be removed. You may also, like we did, want to filter your data to focus on where most of the data lies. Otherwise, you might end up building a model heavily influenced by outliers. In the next chapter, we’ll continue our EDA of diamonds in preparation for modeling by examining the relationship between the four Cs and price. "],["determine_functional_form.html", "3 Determine functional form 3.1 Formulas 3.2 Continuous predictors 3.3 Discrete predictors 3.4 Summary", " 3 Determine functional form library(tidyverse) In Chapter 1, we mentioned that a first step to building a model is to explore your data and choose an appropriate function family. This chapter focuses on this step. Using diamonds, we’ll explore the relationship between price and the four Cs: carat, clarity, color, and cut. By the end of the chapter, we’ll understand which function family or families might approximate this data well. 3.1 Formulas In Chapter 1, we mentioned that the modeling function lm() requires two arguments: a function family and data. With lm(), like many modeling functions, you specify the function family using a formula. Our model from Chapter 1 belonged to the family with the form y = a_0 + a_1 * x_1 where y is the response, x_1 is a continuous variable, and a_0 and a_1 are parameters to be determined. Recall that a_0 is the intercept and a_1 is the slope. This function family is specified with the formula y ~ x_1 . You place the name of the response variable to the left of the ~ and the predictor variable(s) to the right of the ~. The general form for the formula of a linear function of n variables is y ~ x_1 + x_2 + ... + x_n . Formulas are their own class of R object, and you specify them unquoted. y ~ x_1 #&gt; y ~ x_1 class(y ~ x_1) #&gt; [1] &quot;formula&quot; You don’t need to explicitly specify an intercept term as it will be included by default. If you don’t want an intercept term, add a 0 to your formula. y ~ 0 + x_1 We’ll now see how to explore and understand the suitability of modeling your data with linear functions. 3.2 Continuous predictors Let’s start with continuous predictors, as they should be familiar after reading Chapter 1. In Chapter 2, we modified diamonds quite a bit. Here are all of those modifications in one place: df &lt;- diamonds %&gt;% filter(x &gt; 0, y &gt; 0, z &gt; 0) %&gt;% filter(y &lt; 20, z &lt; 10) %&gt;% filter(carat &lt;= quantile(.$carat, probs = 0.99)) %&gt;% mutate(color = fct_rev(color)) Now, we can visualize the relationship between carat and price and look for a linear relationship. df %&gt;% ggplot(aes(carat, price)) + geom_point(alpha = 0.01) The larger diamonds cover a wide range of prices, so we’ll add a smooth line to show the central tendency. df %&gt;% ggplot(aes(carat, price)) + geom_point(alpha = 0.01) + geom_smooth() Notice how price, especially for the larger diamonds, varies substantially among diamonds of the same size. This variation suggests that carat is not the only variable that influences price. From the smooth line, we can also see that the relationship between carat and price is positive, but not linear. Luckily, we can still use a linear model by first transforming the data. We’ll talk more about transformations next. 3.2.1 Transformations Power law relationships take the form y = c * x^d where c and d are both constants. Taking the log of both sides gives us log(y) = log(c) + d * log(x) which might look familiar. Power laws are linear in terms of log(y) and log(x), which is even clearer if we replace log(c) with a_0 and d with a_1. log(y) = a_0 + a_1 * log(x) Phenomena that follow power laws are common. You can determine if your data approximately follow a power law by plotting your data using log scales for the x and y axes and then visually checking if the relationship is linear. df %&gt;% ggplot(aes(carat, price)) + geom_point(alpha = 0.01) + geom_smooth() + scale_x_log10() + scale_y_log10() The smooth line is now linear, suggesting that the functional form log(price) = a_0 + a_1 * log(carat) will approximate the relationship between price and carat well. To specify this function family in a formula, we can apply the log() transformations in the formula: log(price) ~ log(carat) . Log-log transformations are common, but you can apply any transformation (to your response, predictor, or both) that makes your data linear. You might wonder why you should bother transforming your data instead of just fitting a nonlinear model. Linear models are generally simpler, easier to interpret, and often produce better approximations. 3.2.2 Visual checks carat is the only continuous variable of the four Cs, but other datasets will have multiple possible continuous predictors. Here are two features to check for when considering adding a continuous predictor to your model. First, as you’ve already seen, the relationship between the predictor and the response should be linear. If necessary, transform the data so that the relationship is linear. If the relationship is nonlinear and you can’t find a transformation that makes it linear, don’t add the predictor to your linear model. Second, the predictor should influence the response. In our visualization of price and carat, we noticed that price increased as carat increased. If the predictor does not influence the response, you might see something like this: The smooth line is horizontal, which tells you that y does not change with x. The diamonds data is relatively simple, and we’ve made it even simpler by restricting attention to the four Cs. When you have more variables, you’ll need to conduct more preprocessing and pay more attention to which variables to include and which to exclude. If you’d like to learn more, a good resource is Kuhn and Johnson, Feature Engineering and Selection, particularly the Preprocessing section. 3.3 Discrete predictors Earlier, we noticed that price varies substantially for diamonds of a given carat. In this section, we’ll investigate how the three discrete Cs, clarity, color, and cut, influence price. 3.3.1 Functional form Discrete predictors work differently than continuous predictors. When a linear model includes a discrete predictor, a different constant will be added for each value of the variable. We’ll visualize the effect of cut by adding a smooth line for each value to our plot of price vs. carat. We’ll leave off the individual points to focus on the trends.1 plot_smooth &lt;- function(var, data = df) { data %&gt;% ggplot(aes(carat, price, color = {{ var }})) + geom_smooth(method = &quot;loess&quot;, formula = y ~ x) + scale_x_log10() + scale_y_log10() + scale_color_discrete(guide = guide_legend(reverse = TRUE)) } plot_smooth(cut) The smooth lines look linear, and all appear to have similar slopes but different intercepts, which increase as cut quality increases. This suggests that the following functional form, in which the intercept of the line depends on cut, would approximate our data well. log(price) = a_0 + a_1 * log(carat) + f_cut(cut) We need f_cut() to return a different constant for each value of cut. We can write f_cut() in R using case_when(). f_cut &lt;- function(cut) { case_when( cut == &quot;Fair&quot; ~ 0, cut == &quot;Good&quot; ~ a_cut_Good, cut == &quot;Very Good&quot; ~ a_cut_Very_Good, cut == &quot;Premium&quot; ~ a_cut_Premium, cut == &quot;Ideal&quot; ~ a_cut_Ideal ) } Note that the constant for Fair is 0. Fair is the worst quality cut and the lowest factor level of cut. By convention, the lowest factor level’s constant is added to a_0, so we don’t need a a_cut_Fair. The rest of the constants are then created relative to this lowest value. In formula syntax, this functional form is log(price) ~ log(carat) + cut In the next section, we’ll visualize clarity and color and discuss visualizing discrete predictors in more detail. 3.3.2 Visual checks Now that you understand the form of f_cut(), we can be more precise about what to look for when visualizing a potential discrete predictor. Assume you created a visualization like we did above, with a continuous predictor on the x-axis, the response on the y-axis, and a smooth line for each value of the potential discrete predictor. Here’s a set of characteristics to check for: The smooth lines should all be approximately linear. For a linear function to approximate the relationship well, each of the smooth lines needs to be linear. The smooth lines should all be approximately parallel to each other. In our example, the slopes of the lines depend only on our continuous predictor: log(carat). Changing cut does not change the slope. Non-parallel smooth lines suggest that slope depends on both the continuous predictor and the discrete predictor. This is called an interaction. Interactions aren’t part of the set of function families we’re looking at in this chapter, so we won’t talk more about them here. The smooth lines should have different intercepts. Smooth lines that are stacked on top of each other and don’t have different intercepts suggest that the discrete variable doesn’t influence the response. cut met all three of our criteria pretty well, although the Premium and Very Good smooth lines are very close together. plot_smooth(cut) This overlap suggests that there isn’t much of a price difference between Very Good and Premium diamonds. Next, let’s look at clarity. plot_smooth(clarity) clarity also mostly meets our criteria, although two of the levels (VVS1 and VVS2) are again very close together. The slope of the line for I1 also looks a bit different than the rest. However, only 1.28% of diamonds are I1, so it will be okay if our linear model doesn’t approximate them as well as other diamonds. Finally, we’ll look at color. plot_smooth(color) color also meets all three of our criteria pretty well. Again, some of the smooth lines (D, E, F, and G) are almost entirely on top of each other, suggesting that increasing quality above G does not influence price very much. We could try combining D, E, F, and G into one level. df %&gt;% mutate(color_combined = fct_collapse(color, DEFG = c(&quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;))) %&gt;% plot_smooth(color_combined, data = .) The smooth lines are now more clearly separated. Later, when we fit our models, we’ll compare models that combine nearby factor levels to models that use all the original values. The separation between the smooth lines varies between clarity, cut, and color. The larger the separation between smooth lines, the more that variable influences price. So clarity likely influences price the most, followed by color, then cut. 3.4 Summary From our EDA, we now know that: We need to log-transform both price and carat to make the relationship linear. clarity, color, and cut all influence price, and we can add any of them to our model. clarity is the most important of the three discrete Cs, followed by color, then cut. Increasing the quality of the color beyond G doesn’t influence price very much. We could experiment with combining the top levels of color into one level before fitting our model. Even though carat, clarity, color, and cut all influence price, we shouldn’t necessarily add them all into our model. In the next chapter, we’ll explain how to fit a model for a given function family. Then, in the following chapter, we’ll discuss the trade-offs involved in adding additional predictors to models, and we’ll explain how to choose between different models. Note that we used method = \"loess\" instead of the default method = \"gam\". \"loess\" can take more time and memory for large amounts of data, but often gives better results. For more information, see https://dcl-data-vis.stanford.edu/continuous-continuous.html#smoothing .↩︎ "],["fitting_basics.html", "4 Fitting basics 4.1 Fitting a model 4.2 Checking a model 4.3 Summary", " 4 Fitting basics library(tidyverse) library(rstanarm) library(tidymodels) tidymodels_prefer() In Chapters 2 and 3, we used EDA to understand the diamonds dataset and to identify function families with which to model the relationship of price to the four Cs, carat, clarity, color, and cut. We’re now ready to fit models. To start, we’ll use the simplest function family we identified, the power law relationships for price vs. carat, which are expressed by the formula log(price) ~ log(carat) . In the next chapter, we’ll model this data with other function families and show you how to compare and evaluate different models. We’ve made several modifications to the diamonds dataset over the past two chapters, including removing diamonds with impossible dimensions and filtering out very large diamonds. The following code repeats these manipulations and stores the result in a new tibble, df. df &lt;- diamonds %&gt;% filter(x &gt; 0, y &gt; 0, z &gt; 0) %&gt;% filter(y &lt; 20, z &lt; 10) %&gt;% filter(carat &lt;= quantile(.$carat, probs = 0.99)) %&gt;% mutate( color = fct_rev(color), color_combined = fct_collapse(color, &quot;DEFG&quot; = c(&quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;)) ) Going forward, we’ll use df instead of diamonds. 4.1 Fitting a model In Chapter 1, we discussed classical and Bayesian modeling. We’ll now show you how to actually fit classical and Bayesian models, both directly and with the tidymodels modeling framework. 4.1.1 Fitting a classical model The R function lm() fits linear models using classical least squares. Here’s how to use it to fit a model with our function family and data. fit_lm &lt;- lm(log(price) ~ log(carat), data = df) You can learn about the fit by simply printing it. fit_lm #&gt; #&gt; Call: #&gt; lm(formula = log(price) ~ log(carat), data = df) #&gt; #&gt; Coefficients: #&gt; (Intercept) log(carat) #&gt; 8.46 1.69 You can extract the coefficients for the model with coef(). coef(fit_lm) #&gt; (Intercept) log(carat) #&gt; 8.46 1.69 4.1.2 Fitting a Bayesian model Stan is a platform for statistical modeling, including Bayesian modeling. rstanarm is an “R package that emulates other R model-fitting functions but uses Stan … for the back-end estimation.” stan_glm() is an rstanarm function that emulates the R function glm() to fit generalized linear models. Here’s how to use it to fit a model with our function family and data. fit_stan_glm &lt;- stan_glm(log(price) ~ log(carat), data = df, refresh = 0, seed = 505) stan_glm() uses a probabilistic algorithm. If you wish, you can use the seed argument to set the starting point of the random number generator used by the algorithm so that it will return the same results every time it is called. The refresh = 0 argument suppresses information that the algorithm would otherwise print while it is running. You can learn about the fit by simply printing it. fit_stan_glm #&gt; stan_glm #&gt; family: gaussian [identity] #&gt; formula: log(price) ~ log(carat) #&gt; observations: 53405 #&gt; predictors: 2 #&gt; ------ #&gt; Median MAD_SD #&gt; (Intercept) 8.5 0.0 #&gt; log(carat) 1.7 0.0 #&gt; #&gt; Auxiliary parameter(s): #&gt; Median MAD_SD #&gt; sigma 0.3 0.0 #&gt; #&gt; ------ #&gt; * For help interpreting the printed output see ?print.stanreg #&gt; * For info on the priors used see ?prior_summary.stanreg By default, this only prints one digit of precision. Here’s how you can print more. print(fit_stan_glm, digits = 3) #&gt; stan_glm #&gt; family: gaussian [identity] #&gt; formula: log(price) ~ log(carat) #&gt; observations: 53405 #&gt; predictors: 2 #&gt; ------ #&gt; Median MAD_SD #&gt; (Intercept) 8.457 0.001 #&gt; log(carat) 1.688 0.002 #&gt; #&gt; Auxiliary parameter(s): #&gt; Median MAD_SD #&gt; sigma 0.260 0.001 #&gt; #&gt; ------ #&gt; * For help interpreting the printed output see ?print.stanreg #&gt; * For info on the priors used see ?prior_summary.stanreg The parameter sigma is an estimate of the standard deviation of the residual distribution, which is assumed to be normal with mean 0. The MAD_SD column is the scaled median absolute deviation, which provides a robust measure of variation of the parameter distributions. An advantage of Bayesian modeling is that it provides estimates not just of the values of the parameters, but also an estimate of their joint distribution through simulations. Here’s how you can access these simulations. sims &lt;- as.data.frame(fit_stan_glm) %&gt;% as_tibble() By default, stan_glm() performs 4000 simulations from the joint distribution of parameters. sims #&gt; # A tibble: 4,000 × 3 #&gt; `(Intercept)` `log(carat)` sigma #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 8.46 1.68 0.260 #&gt; 2 8.46 1.69 0.262 #&gt; 3 8.46 1.69 0.262 #&gt; 4 8.46 1.69 0.261 #&gt; 5 8.46 1.69 0.261 #&gt; 6 8.46 1.69 0.261 #&gt; # … with 3,994 more rows These simulations can be used to make inferences about the model, such as uncertainties in the parameters or in the model’s predictions. You can extract the coefficients for the model with coef(). coef(fit_stan_glm) #&gt; (Intercept) log(carat) #&gt; 8.46 1.69 The coefficients produced by lm() and stan_glm() are very close. coef(fit_lm) - coef(fit_stan_glm) #&gt; (Intercept) log(carat) #&gt; -4.91e-05 -5.11e-05 4.1.3 Tidymodels Tidymodels is an ecosystem of R packages for modeling. It provides a common interface to a growing number of model types and engines. And it supports a range of other modeling tasks, such as data preprocessing, resampling, and parameter tuning. You can learn more at Tidy Modeling with R. Linear regression, the type of model we’ll be using here, is just one of several model types supported by tidymodels. Each type of model supported by tidymodels may have multiple computational engines. Here are the engines currently available for linear regression. show_engines(&quot;linear_reg&quot;) #&gt; # A tibble: 7 × 2 #&gt; engine mode #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 lm regression #&gt; 2 glm regression #&gt; 3 glmnet regression #&gt; 4 stan regression #&gt; 5 spark regression #&gt; 6 keras regression #&gt; # … with 1 more row Tidymodels provides a common interface for each type of model, so you don’t need to know the details and arguments for each computational engine. This makes it easier to model a dataset using different types of model, and using different computational engines for the same type of model. Here’s how to fit our function family and data with tidymodels using lm(). fit_tm_lm &lt;- linear_reg() %&gt;% set_engine(&quot;lm&quot;) %&gt;% fit(log(price) ~ log(carat), data = df) fit_tm_lm #&gt; parsnip model object #&gt; #&gt; #&gt; Call: #&gt; stats::lm(formula = log(price) ~ log(carat), data = data) #&gt; #&gt; Coefficients: #&gt; (Intercept) log(carat) #&gt; 8.46 1.69 The tidymodels fit contains the lm() fit. fit_tm_lm$fit #&gt; #&gt; Call: #&gt; stats::lm(formula = log(price) ~ log(carat), data = data) #&gt; #&gt; Coefficients: #&gt; (Intercept) log(carat) #&gt; 8.46 1.69 Tidymodels uses stan_glm() to fit linear regressions with the stan engine. Here’s how to fit our function family and data with tidymodels using stan_glm(). fit_tm_stan &lt;- linear_reg() %&gt;% set_engine(&quot;stan&quot;, seed = 505) %&gt;% fit(log(price) ~ log(carat), data = df) The seed argument isn’t required. We used it so the results would match the direct call of stan_glm() above. With tidymodels, refresh = 0 is the default. Here’s information about the fit. fit_tm_stan #&gt; parsnip model object #&gt; #&gt; stan_glm #&gt; family: gaussian [identity] #&gt; formula: log(price) ~ log(carat) #&gt; observations: 53405 #&gt; predictors: 2 #&gt; ------ #&gt; Median MAD_SD #&gt; (Intercept) 8.5 0.0 #&gt; log(carat) 1.7 0.0 #&gt; #&gt; Auxiliary parameter(s): #&gt; Median MAD_SD #&gt; sigma 0.3 0.0 #&gt; #&gt; ------ #&gt; * For help interpreting the printed output see ?print.stanreg #&gt; * For info on the priors used see ?prior_summary.stanreg The tidymodels fit contains the stan_glm() fit. print(fit_tm_stan$fit, digits = 3) #&gt; stan_glm #&gt; family: gaussian [identity] #&gt; formula: log(price) ~ log(carat) #&gt; observations: 53405 #&gt; predictors: 2 #&gt; ------ #&gt; Median MAD_SD #&gt; (Intercept) 8.457 0.001 #&gt; log(carat) 1.688 0.002 #&gt; #&gt; Auxiliary parameter(s): #&gt; Median MAD_SD #&gt; sigma 0.260 0.001 #&gt; #&gt; ------ #&gt; * For help interpreting the printed output see ?print.stanreg #&gt; * For info on the priors used see ?prior_summary.stanreg 4.2 Checking a model In the last chapter, we used EDA to find promising function families with which to model the diamonds dataset. We can now use EDA to check to see how well a model fits the data. Here’s a plot of the data we’re modeling and a smooth line. df %&gt;% ggplot(aes(carat, price)) + geom_point(alpha = 0.01) + geom_smooth() Recall that the formula for our function family log(price) ~ log(carat) represents functions of the form log(price) = a_0 + a_1 * log(carat) for parameters a_0 and a_1. From the coefficients we saw above, the algorithms chose the function log(price) = 8.46 + 1.69 * log(carat) or, after we apply exp() to both sides, price = 4707 * carat^1.69 . The smooth line above curves upward, indicating a growth in price that is greater than linear. The model exponent is 1.69. Since this is larger than 1, it likewise represents a growth greater than linear. This function predicts that a one-carat diamond would have a price of about 4707. Here’s the distribution of actual prices of one-carat diamonds. df %&gt;% filter(near(carat, 1)) %&gt;% pull(price) %&gt;% summary() #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 1681 4155 4864 5243 6079 16469 As you can see from the plot and the summary, there is a wide variation in prices for one-carat diamonds, but the prediction is roughly comparable with the median and mean prices. With no glaring problems with the parameters, we’ll next look at predictions across the full range of carat. 4.2.1 Making predictions Once you’ve fit a model, you can use the fit to make predictions. Here’s how to use the predict() function to make predictions for the lm() and stan_glm() models. preds &lt;- tibble( carat = seq(min(df$carat), max(df$carat), length.out = 801), pred_lm = predict(fit_lm, newdata = tibble(carat)) %&gt;% exp(), pred_stan_glm = predict(fit_stan_glm, newdata = tibble(carat)) %&gt;% exp() ) preds #&gt; # A tibble: 801 × 3 #&gt; carat pred_lm pred_stan_glm #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.2 311. 311. #&gt; 2 0.202 317. 317. #&gt; 3 0.205 324. 324. #&gt; 4 0.207 331. 331. #&gt; 5 0.210 337. 337. #&gt; 6 0.212 344. 344. #&gt; # … with 795 more rows In the tibble above, carat contains equally spaced values. We then use the newdata argument of predict() to make predictions at these values using the models. predict() makes predictions of log(price), since this was the response in the formula defining the model function family. We therefore use exp() to get predictions for price. lm() is from the stats package, and stan_glm() is from the rstanarm package. These packages were written and are maintained by different people. You might wonder how the function predict() could make predictions for such very different models from these two different packages. It does this by taking advantage for R’s object-oriented functionality. fit_lm is has class lm. class(fit_lm) #&gt; [1] &quot;lm&quot; When predict() encounters a model of this class, it calls predict.lm() from the stats package. fit_stan_glm has class stanreg. class(fit_stan_glm) #&gt; [1] &quot;stanreg&quot; &quot;glm&quot; &quot;lm&quot; When predict() encounters a model of this class, it calls predict.stanreg() from the rstanarm package. The tidymodels models fit_tm_lm and fit_tm_stan have class model_fit class(fit_tm_lm) #&gt; [1] &quot;_lm&quot; &quot;model_fit&quot; class(fit_tm_stan) #&gt; [1] &quot;_stanreg&quot; &quot;model_fit&quot; When predict() encounters a model of this class, it calls predict.model_fit() from the tidymodels parsnip package. We need to use slightly different arguments to make predictions with this function. preds &lt;- preds %&gt;% mutate( pred_tm_lm = predict(fit_tm_lm, new_data = tibble(carat), type = &quot;raw&quot;) %&gt;% exp(), pred_tm_stan = predict(fit_tm_stan, new_data = tibble(carat), type = &quot;raw&quot;) %&gt;% exp() ) preds #&gt; # A tibble: 801 × 5 #&gt; carat pred_lm pred_stan_glm pred_tm_lm pred_tm_stan #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.2 311. 311. 311. 311. #&gt; 2 0.202 317. 317. 317. 317. #&gt; 3 0.205 324. 324. 324. 324. #&gt; 4 0.207 331. 331. 331. 331. #&gt; 5 0.210 337. 337. 337. 337. #&gt; 6 0.212 344. 344. 344. 344. #&gt; # … with 795 more rows The predictions by the models created with tidymodels are exactly the same as those created directly with lm() and stan_glm(). preds %&gt;% summarize( diff_lm = max(abs(pred_tm_lm - pred_lm)), diff_stan_glm = max(abs(pred_tm_stan - pred_stan_glm)) ) #&gt; # A tibble: 1 × 2 #&gt; diff_lm diff_stan_glm #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 0 And the predictions by the models created by directly lm() and stan_glm() are very close. v &lt;- preds %&gt;% summarize( rel_max = max(abs(pred_stan_glm - pred_lm) / pmin(pred_stan_glm, pred_lm)) ) v #&gt; # A tibble: 1 × 1 #&gt; rel_max #&gt; &lt;dbl&gt; #&gt; 1 0.0000659 The predictions are always within 0.0066% of each other. Since the predictions of the different models are all very close, in the following we will only check the fit of stan_glm(). 4.2.2 Checking predictions Recall from Chapter 3 that log-log plots are an effective way to plot data from a power law. On a log-log plot, a power law function is linear. Here’s our data on a log-log plot with a smooth line and our model’s predictions. df %&gt;% ggplot(aes(carat, price)) + geom_point(alpha = 0.01) + geom_smooth(aes(color = &quot;Data&quot;)) + geom_line(aes(y = pred_stan_glm, color = &quot;Model&quot;), data = preds, size = 1) + scale_x_log10() + scale_y_log10() + scale_color_discrete(direction = -1) + theme(legend.position = &quot;top&quot;) + labs(color = NULL) For diamonds greater than or equal to 0.3 carat in size, the model predictions are very close to the smooth line. For diamonds smaller than 0.3 carat, the actual prices of diamonds are greater than the prices predicted by the model. However, there are relatively few diamonds of this size. Since stan_glm() is a Bayesian model, we can plot estimates of the uncertainty in its predictions. predictive_intervals &lt;- function(.data, fit, probs = c(0.5, 0.9)) { .data %&gt;% mutate(.pred = predict(fit, newdata = .)) %&gt;% bind_cols( map_dfc( probs, ~ predictive_interval(fit, prob = ., newdata = .data) %&gt;% as_tibble() ) ) } v &lt;- tibble(carat = seq(min(df$carat), max(df$carat), length.out = 801)) %&gt;% predictive_intervals(fit = fit_stan_glm) %&gt;% mutate(across(!carat, exp)) v %&gt;% ggplot(aes(carat)) + geom_ribbon(aes(ymin = `5%`, ymax = `95%`), alpha = 0.25) + geom_ribbon(aes(ymin = `25%`, ymax = `75%`), alpha = 0.5) + geom_point(aes(y = price), data = df, alpha = 0.01) + geom_line(aes(y = .pred), color = &quot;#f8766d&quot;, size = 1) + coord_cartesian(ylim = c(min(df$price), max(df$price))) + scale_x_log10() + scale_y_log10() + labs( title = &quot;Bayesian linear regression with 50% and 90% predictive intervals&quot; ) The 90% predictive interval contains the bulk of the data. With no major discrepancies between the model’s predictions and the data, we’ll now turn to a more sophisticated technique from EDA – checking the model’s residuals. 4.2.3 Checking residuals We encountered residuals in Chapter 1. For a point in a dataset, the residual at that point is the true value of the response variable minus the value predicted using the corresponding predictor variable(s). Our formula uses logs, so the residual for a logged response is log(price) - log(predicted price) or equivalently log(price / predicted price) . Applying exp() to this residual turns an additive error into a multiplicative error. This is the natural way to study error for a power law, where the range of the response variable can vary greatly. For example, in our dataset price ranges from $326 to $18,818. We will therefore examine the ratio price / predicted price . If we call the predict() function without specifying new data, it will return the predictions for the dataset used to fit the model. v &lt;- df %&gt;% select(price, carat) %&gt;% mutate( pred = predict(fit_stan_glm) %&gt;% exp(), error_ratio = price / pred ) v #&gt; # A tibble: 53,405 × 4 #&gt; price carat pred error_ratio #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 326 0.23 394. 0.828 #&gt; 2 326 0.21 338. 0.965 #&gt; 3 327 0.23 394. 0.831 #&gt; 4 334 0.29 582. 0.574 #&gt; 5 335 0.31 652. 0.514 #&gt; 6 336 0.24 423. 0.794 #&gt; # … with 53,399 more rows Let’s look at a plot of the error ratios. v %&gt;% ggplot(aes(carat, error_ratio)) + geom_point(alpha = 0.01) + geom_hline(yintercept = 1, size = 1, color = &quot;white&quot;) + geom_smooth() + coord_cartesian(ylim = c(0, NA)) + scale_x_log10() + labs(y = &quot;Error ratio&quot;) As we saw above, the actual prices of the diamonds were more expensive than the model’s predictions for those with size less that 0.3 carat. In addition, we can now see that diamonds around 1.0 and 1.5 carats were also more expensive than predicted. Nevertheless, the smooth line for error_ratio is remains fairly close to 1 for diamonds of size 0.3 carat and greater. A systematic divergence from this line would indicate a problem with the fit. Let’s look at the distribution of error_ratio. quantile(v$error_ratio, probs = c(0.025, 0.05, 0.5, 0.95, 0.975)) #&gt; 2.5% 5% 50% 95% 97.5% #&gt; 0.614 0.662 0.994 1.538 1.681 The median error_ratio is very close to 1. Approximately 95% of the diamonds are within the range of 39% less and 68% more than the predictions. 4.3 Summary The process of fitting classical and Bayesian models is similar. Bayesian models have the advantage of enabling additional inferences about the model, such as uncertainties in the parameters or the model’s predictions. Tidymodels provides a common interface to a wide range of model types and engines, as well as other tools for the modeling process. Once we have fit a model, we can check the fit by: Checking the model parameters against known features of the data. Checking the predictions of the model against the actual values of the response variable in the data. Checking the residuals to see if their smooth line is largely horizontal. These checks only indicate how well the model fits the data. They are not an indication of how well the model would make predictions with new data. We will turn to that question in the next chapter, as well as the issue of how to evaluate and compare models from different function families. "],["model_evaluation.html", "5 Model evaluation 5.1 Model overfitting 5.2 Model selection 5.3 Final model 5.4 Summary 5.5 To learn more", " 5 Model evaluation library(tidyverse) library(dcldata) library(robustbase) library(rstanarm) library(tidymodels) tidymodels_prefer() After you’ve used EDA to understand your data and identify potential function families with which to model, you can fit models for each of these function families. In this chapter, we’ll show you how to compare and evaluate the resulting models. Our exploration in Chapter 2 led us to focus on the following subset of the diamonds dataset, which we will use below. In addition, we will convert the ordered factors into unordered factors so that their model coefficients will be interpretable. df &lt;- diamonds %&gt;% filter(x &gt; 0, y &gt; 0, z &gt; 0) %&gt;% filter(y &lt; 20, z &lt; 10) %&gt;% filter(carat &lt;= quantile(.$carat, probs = 0.99)) %&gt;% mutate( color = fct_rev(color), color_combined = fct_collapse(color, &quot;DEFG&quot; = c(&quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;)), across(where(is.ordered), ~ factor(., ordered = FALSE)) ) In Chapter 3, we explored the relationship of price to the four Cs: carat, clarity, color, and cut. We saw that the relationship of price to the continuous variable carat could be approximated with a power law. We saw that the other three the discrete variables were suitable for inclusion into a linear model, with clarity being the most important, followed by color, and then cut. Finally, we saw that the colors with quality greater than G didn’t influence price much, which led to the color_combined variable where we collapsed the colors D, E, F, and G into one value. We can therefore consider the following function families with which to model. formulas &lt;- tribble( ~formula, &quot;log(price) ~ log(carat)&quot;, &quot;log(price) ~ log(carat) + clarity&quot;, &quot;log(price) ~ log(carat) + clarity + color_combined&quot;, &quot;log(price) ~ log(carat) + clarity + color&quot;, &quot;log(price) ~ log(carat) + clarity + color + cut&quot; ) %&gt;% rowwise() %&gt;% mutate( n_coefs = model.matrix(as.formula(formula), data = df %&gt;% slice(1)) %&gt;% ncol() ) %&gt;% ungroup() knitr::kable(formulas) formula n_coefs log(price) ~ log(carat) 2 log(price) ~ log(carat) + clarity 9 log(price) ~ log(carat) + clarity + color_combined 12 log(price) ~ log(carat) + clarity + color 15 log(price) ~ log(carat) + clarity + color + cut 19 The n_coefs column contains the number of coefficients or parameters for the function family and ranges from two, for the simplest family we used in last chapter, to 19 for the most complex function family. Each function family in the list includes all of the functions in the function families above it. You might ask, why not just use the most complex function family, since it contains all the functions of the simpler families? We’ll explain why in the next section. But before we can discuss model evaluation, we need to specify the purpose for our modeling. We will assume that our goal is to create a model using the data we have to make accurate predictions on new data. 5.1 Model overfitting For this section, we will seek to model blood pressure as a function of age. The data we’ll use are from the National Health and Nutrition Examination Survey (NHANES) 2017-2018 and are in the dcldata package. blood_pressure #&gt; # A tibble: 6,144 × 5 #&gt; seqn ridageyr bpxosy1 bpxosy2 bpxosy3 #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 93705 66 164 165 172 #&gt; 2 93706 18 126 128 133 #&gt; 3 93707 13 136 133 139 #&gt; 4 93708 66 146 142 151 #&gt; 5 93709 75 120 124 113 #&gt; 6 93711 56 112 112 109 #&gt; # … with 6,138 more rows Here are the variables: seqn: Respondent sequence number ridageyr: Age in years at screening. All participants aged 80 years and older are coded as 80. bpxosy1: Systolic blood pressure - 1st oscillometric reading bpxosy2: Systolic blood pressure - 2nd oscillometric reading bpxosy3: Systolic blood pressure - 3rd oscillometric reading Since we don’t know the actual age for participants with rigageyr variable equal to 80, we will only consider those age 79 and younger. The blood pressure variable we’ll model will be a new variable bpxosy that is the mean of the three systolic blood pressure measurements. bp_all &lt;- blood_pressure %&gt;% filter(ridageyr &lt;= 79) %&gt;% rowwise() %&gt;% mutate(bpxosy = mean(c(bpxosy1, bpxosy2, bpxosy3), na.rm = TRUE)) %&gt;% ungroup() %&gt;% select(!matches(&quot;bpxosy\\\\d&quot;)) %&gt;% arrange(seqn) bp_all #&gt; # A tibble: 5,851 × 3 #&gt; seqn ridageyr bpxosy #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 93705 66 167 #&gt; 2 93706 18 129 #&gt; 3 93707 13 136 #&gt; 4 93708 66 146. #&gt; 5 93709 75 119 #&gt; 6 93711 56 111 #&gt; # … with 5,845 more rows n &lt;- 200 We’ll now create a random sample of 200 participants, bp_model, and we’ll assume that this sample is all the data we have to work with to create our model. We’ll use the remaining data, bp_new, to judge how well the model would predict on new data. set.seed(950) bp_model &lt;- bp_all %&gt;% sample_n(size = n) %&gt;% arrange(seqn) bp_new &lt;- bp_all %&gt;% anti_join(bp_model, by = &quot;seqn&quot;) %&gt;% arrange(seqn) bp_model and bp_new partition bp_all into two disjoint datasets. is_empty(intersect(bp_model$seqn, bp_new$seqn)) #&gt; [1] TRUE setequal(bp_all$seqn, union(bp_model$seqn, bp_new$seqn)) #&gt; [1] TRUE Our first step is to use EDA to understand the data and potential function families with which to model it. bp_model %&gt;% ggplot(aes(ridageyr, bpxosy)) + geom_point() + geom_smooth() + labs( title = &quot;Smooth line&quot;, x = &quot;Age (years)&quot;, y = &quot;Systolic blood pressure (mm Hg)&quot; ) We can see that the smooth line of the data is quite linear, indicating that linear functions could be a promising function family with which to model the data. We can also see from the data that we should definitely not use lm() or any other method that uses the least squares algorithm. The least squares algorithm assumes that the errors or residuals are normally distributed with a constant standard deviation. The data has a wide dispersion and outliers that are not consistent with a normal distribution, and the variation in the data appears to increase with increasing age. For these reasons, we will model this data with the function lmrob() from the robustbase package. This function is designed to work with data where the least squares assumptions do not hold. Though the function family of linear functions seems promising for this data, we’d like to explore the use of more complex functions. For this purpose, we will use higher degree polynomials. Unless your understanding of the data calls for a polynomial, these are rarely a good choice for modeling function families, especially higher degree polynomials, but we are choosing them to illustrate a point. Let’s first use EDA to see the models from the function families of polynomials up to degree six. plot_poly &lt;- function(degree) { predictions &lt;- tibble( ridageyr = seq(min(bp_model$ridageyr), max(bp_model$ridageyr), length.out = 201), bpxosy = lmrob( bpxosy ~ poly(ridageyr, degree = degree), data = bp_model ) %&gt;% predict(newdata = tibble(ridageyr)) ) bp_model %&gt;% ggplot(aes(ridageyr, bpxosy)) + geom_point() + geom_line(data = predictions, color = &quot;blue&quot;, size = 1) + labs( title = str_glue(&quot;Polynomial model of degree {degree}&quot;), x = &quot;Age (years)&quot;, y = &quot;Systolic blood pressure (mm Hg)&quot; ) } 1:6 %&gt;% map(plot_poly) %&gt;% walk(print) The linear and quadratic models are very similar. As the degree of the polynomials increases, the fits seem increasingly implausible. Recall that our goal is to create a model that will make accurate predictions on new data. We have new data in bp_new, which lmrob() did not see when creating the models. So we can make predictions on this new data and compare the predictions to the actual measured blood pressures. Recall from Chapter 1 that the differences between the predictions and the actual response are called residuals. There are different metrics for measuring the residuals, including the root mean squared error (RMSE) sqrt(mean(residuals^2)) and the mean absolute error (MAE) mean(abs(residuals)) . Since our data has outliers, we will use the more robust mean absolute error. Here’s the calculation of the MAE for the predictions of the six models above on the new data in bp_new. bp_errors &lt;- tibble(formula = str_glue(&quot;bpxosy ~ poly(ridageyr, degree = {1:6})&quot;)) %&gt;% rowwise() %&gt;% mutate( n_coefs = model.matrix(as.formula(formula), data = bp_model) %&gt;% ncol(), model = list(lmrob(as.formula(formula), data = bp_model)), mae = mean(abs(bp_new$bpxosy - predict(model, newdata = bp_new))) ) %&gt;% ungroup() %&gt;% select(!model) And here’s a plot of the MAE against the number of model coefficients, a measure of model complexity. bp_errors %&gt;% ggplot(aes(n_coefs, mae)) + geom_line() + geom_point() + scale_y_continuous(labels = scales::label_number(accuracy = 0.01)) + labs( title = &quot;Mean absolute error of model predictions on new data&quot;, x = &quot;Number of model coefficients&quot;, y = &quot;Mean absolute error&quot; ) The simplest model, the linear model, had the best performance on new data, and the performance of the models worsened with increasing model complexity. Why did the more complex functions have worse performance? Consider the relationship of our sample data to new data. For each participant age, we can imagine that the blood pressure measurements in our random sample are a random sample of blood pressures from the population for those of that age. We saw that our data had considerable noise, so the random sample may not have been representative of the mean or median value of the population blood pressures for a given age. The more complexity a function family has, the greater its ability to fit the noise. Using a function family with too much complexity that results in fitting the noise is called overfitting. A model that is overfit will not generalize and work well on new data. Model simplicity is a virtue. If the estimated predictive performance of two models is comparable, the simpler of the two is typically preferred, since it is more likely to generalize and work well on new data. 5.2 Model selection We’ve chosen five function families with which to model the diamonds data. To select between the five resulting models, we will now estimate their predictive performance on new data. Classical modeling and Bayesian modeling take different approaches to this estimation. 5.2.1 Classical modeling In classical modeling, estimates of the predictive performance of a model are typically made using cross-validation. The fundamental idea of cross-validation is to train your model using a portion of your original data, and to then measure the model’s predictions on the remaining data. The partitioning of the data into two disjoint sets can be done in different ways, common methods include: Monte Carlo: The training set is a random sample without replacement of the original data, and the test set is the remaining data. The modeler specifies the proportion of original data to use in the training set, typically between 75 - 80%. Bootstrap: The training set is a random sample with replacement of the original data for a resulting set the same size as the original data. The test set is the remaining data. In bootstrap, the proportion of original data used in the training set is approximately 63.2%. V-Fold: In this method, the original data is randomly partitioned into v disjoint sets of roughly equal size, called folds. The modeling is then done v times with each fold serving as test set and the remaining v - 1 folds serving as the training set. Typical values for v are 5 or 10, corresponding to 80% or 90% of the original data being used for the training set. With Monte Carlo and bootstrap cross-validation, the train and test process is repeated multiple times with different random partitions. With v-fold cross-validation, the whole process of v repetitions can itself be repeated. In each of the three methods, the results of the repetitions are combined into an average estimate of predictive performance and a standard error. For our diamonds data, we’ll use v-fold cross-validation with v = 10 folds and just one repeat. The vfold_cv() function in the tidymodels rsample package will do this for us. set.seed(616) resamples &lt;- vfold_cv(data = df, v = 10) resamples #&gt; # 10-fold cross-validation #&gt; # A tibble: 10 × 2 #&gt; splits id #&gt; &lt;list&gt; &lt;chr&gt; #&gt; 1 &lt;split [48064/5341]&gt; Fold01 #&gt; 2 &lt;split [48064/5341]&gt; Fold02 #&gt; 3 &lt;split [48064/5341]&gt; Fold03 #&gt; 4 &lt;split [48064/5341]&gt; Fold04 #&gt; 5 &lt;split [48064/5341]&gt; Fold05 #&gt; 6 &lt;split [48065/5340]&gt; Fold06 #&gt; # … with 4 more rows Each row of this tibble contains a partition or split of the data into two disjoint sets, one to train a model and the other to test the resulting model. For example, the first row contains a split of the 53,405 rows of df into 48,064 rows to fit or train a model, and the remaining 5,341 rows to test the resulting model. The fit_resamples() function in the tidymodels tune package streamlines the process of fitting and testing a model for all rows in a set of resamples. Given a model specification and a set of resamples, this function will, for each row: Fit a model using the split training set. Make predictions using the split test set. Calculate the residuals of the predictions. Calculate a metric of the residuals; for example, the RMSE or MAE. Of course, what we’re really interested in is not the individual metric for each resample model but rather a mean value of the metric for all the resample models and a standard error. This can be accomplished with the collect_metrics() function in the tidymodels tune package. Putting this all together, we can get an estimate for how a model would perform on new data. For example, here’s an estimate of how the model produced by lm() for our simplest function family would perform on new data using the RMSE metric. model_metric &lt;- function(formula, engine = &quot;lm&quot;, metric = rmse) { workflow() %&gt;% add_model(linear_reg() %&gt;% set_engine(engine)) %&gt;% add_formula(as.formula(formula)) %&gt;% fit_resamples(resamples = resamples, metrics = metric_set(metric)) %&gt;% collect_metrics() } v &lt;- model_metric(log(price) ~ log(carat)) v #&gt; # A tibble: 1 × 6 #&gt; .metric .estimator mean n std_err .config #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 rmse standard 0.260 10 0.00130 Preprocessor1_Model1 After fitting and testing resample models for each of the 10 rows of resamples and collecting the results, our RSME estimate for predictive performance is about 0.260 with a standard error of 0.00130. This cross-validation estimate of how a model will perform on new data allows us to compare models from different function families. Let’s first calculate the estimates for all five of the function families we’re considering. rmse_lm &lt;- formulas %&gt;% rowwise() %&gt;% mutate(model_metric(formula)) %&gt;% ungroup() %&gt;% select(formula, n_coefs, rmse = mean, rmse_se = std_err) knitr::kable(rmse_lm) formula n_coefs rmse rmse_se log(price) ~ log(carat) 2 0.260 0.001 log(price) ~ log(carat) + clarity 9 0.187 0.001 log(price) ~ log(carat) + clarity + color_combined 12 0.144 0.001 log(price) ~ log(carat) + clarity + color 15 0.136 0.001 log(price) ~ log(carat) + clarity + color + cut 19 0.132 0.001 Let’s visualize the results. p &lt;- rmse_lm %&gt;% ggplot(aes(n_coefs, rmse)) + geom_line() + geom_linerange(aes(ymin = rmse - rmse_se, ymax = rmse + rmse_se)) + geom_point() + geom_text( aes(label = formula), hjust = c(0, 0, 1, 1, 1), nudge_x = 0.2 * c(1, 1, -1, -1, -1) ) + scale_x_continuous( breaks = seq(min(rmse_lm$n_coefs), max(rmse_lm$n_coefs)), minor_breaks = NULL ) + labs( title = &quot;Cross-validation estimate of predictive performance&quot;, x = &quot;Number of model coefficients&quot;, y = &quot;Root mean squared error&quot; ) p With the blood pressure models, the predictive performance worsened with increasing model complexity. In contrast, with the diamonds models, the estimated predictive performance improves with increasing model complexity. An explanation for this is the difference in the amount of data in the two datasets. The blood pressure data has 200 rows, whereas the diamonds data has 53,405 rows. The more data you have to work with, the greater the complexity of the models that you can typically use before the predictive performance degrades due to overfitting. Let’s zoom in on the three models with the best predictive performance. p + coord_cartesian(ylim = c(NA, 0.15)) At this scale, we can see the error bars for one standard error on either side of the estimate. A rule of thumb for selecting a model when using cross-validation is to choose the model with the lowest complexity whose estimated predictive performance is within one standard error of the best predictive performance. In this case, no simpler model has a performance within one standard error of the model with the best performance, so we would choose the most complex model log(price) ~ log(carat) + clarity + color + cut . 5.2.2 Bayesian modeling Instead of using cross-validation, Bayesian modeling takes a different approach. When Stan fits a Bayesian model, the fitting process itself generates information that can be used to estimate the model’s predictive performance without having to refit the model multiple times on resampled data. The loo() function in the loo package provides an approximation to leave-one-out (LOO) cross-validation using just the information from a Stan model fit. LOO cross-validation is the special case of k-fold cross-validation when k = n, the number of rows in the data. In other words, LOO cross-validation would require fitting n models, each on all data except for a single point, and then testing each resulting model on the omitted point. loo() can estimate LOO cross-validation without having to fit any additional models. Here’s the estimate using loo() of how the model produced by stan_glm() for our simplest function family would perform on new data. set.seed(983) model_loo &lt;- function(formula, ...) { stan_glm(as.formula(formula), data = df, refresh = 0) %&gt;% loo(...) } loo_1 &lt;- model_loo(formulas$formula[1]) loo_1 #&gt; #&gt; Computed from 4000 by 53405 log-likelihood matrix #&gt; #&gt; Estimate SE #&gt; elpd_loo -3931.4 190.0 #&gt; p_loo 3.4 0.0 #&gt; looic 7862.7 380.0 #&gt; ------ #&gt; Monte Carlo SE of elpd_loo is 0.0. #&gt; #&gt; All Pareto k estimates are good (k &lt; 0.5). #&gt; See help(&#39;pareto-k-diagnostic&#39;) for details. elpd_loo is an estimate of the expected log predictive density (ELPD) – this is an estimate of the predictive performance of the model on new data, and we can use it to compare models from different function families. With ELPD, larger values indicate better estimated predictive performance. The line All Pareto k estimates are good (k &lt; 0.5). is a diagnostic for the estimation process. In this case, everything worked fine. If this were not the case, the message would recommend corrective action, such as calling loo() again with arguments other than the defaults. With ELPD as our measure of estimated predictive performance, we’ll now calculate it for the remaining four function families we’re considering. set.seed(983) loo_2 &lt;- model_loo(formulas$formula[2]) loo_2 #&gt; #&gt; Computed from 4000 by 53405 log-likelihood matrix #&gt; #&gt; Estimate SE #&gt; elpd_loo 13713.2 187.7 #&gt; p_loo 11.6 0.3 #&gt; looic -27426.3 375.3 #&gt; ------ #&gt; Monte Carlo SE of elpd_loo is 0.1. #&gt; #&gt; All Pareto k estimates are good (k &lt; 0.5). #&gt; See help(&#39;pareto-k-diagnostic&#39;) for details. The ELPD for model 2 is substantially better than that for model 1, so it would be the better choice. set.seed(983) loo_3 &lt;- model_loo(formulas$formula[3]) loo_3 #&gt; #&gt; Computed from 4000 by 53405 log-likelihood matrix #&gt; #&gt; Estimate SE #&gt; elpd_loo 27673.3 218.6 #&gt; p_loo 15.6 0.5 #&gt; looic -55346.7 437.2 #&gt; ------ #&gt; Monte Carlo SE of elpd_loo is 0.1. #&gt; #&gt; All Pareto k estimates are good (k &lt; 0.5). #&gt; See help(&#39;pareto-k-diagnostic&#39;) for details. The ELPD for the model 3 is substantially better than that for model 2. set.seed(983) loo_4 &lt;- model_loo(formulas$formula[4]) loo_4 #&gt; #&gt; Computed from 4000 by 53405 log-likelihood matrix #&gt; #&gt; Estimate SE #&gt; elpd_loo 30636.0 226.5 #&gt; p_loo 19.0 0.7 #&gt; looic -61272.0 453.1 #&gt; ------ #&gt; Monte Carlo SE of elpd_loo is 0.1. #&gt; #&gt; All Pareto k estimates are good (k &lt; 0.5). #&gt; See help(&#39;pareto-k-diagnostic&#39;) for details. The ELPD for the model 4 is an improvement over that for model 3, but not as much of an improvement as with the earlier models. set.seed(983) loo_5 &lt;- model_loo(formulas$formula[5]) loo_5 #&gt; #&gt; Computed from 4000 by 53405 log-likelihood matrix #&gt; #&gt; Estimate SE #&gt; elpd_loo 32266.8 245.7 #&gt; p_loo 24.0 1.0 #&gt; looic -64533.6 491.4 #&gt; ------ #&gt; Monte Carlo SE of elpd_loo is 0.1. #&gt; #&gt; All Pareto k estimates are good (k &lt; 0.5). #&gt; See help(&#39;pareto-k-diagnostic&#39;) for details. The ELPD for model 5 is again a modest improvement over that for model 4. We’ll gather the results into a tibble. loos &lt;- list(loo_1, loo_2, loo_3, loo_4, loo_5) elpd_stan_glm &lt;- formulas %&gt;% mutate( elpd = map_dbl(loos, ~ pluck(., &quot;estimates&quot;)[&quot;elpd_loo&quot;, &quot;Estimate&quot;]), elpd_se = map_dbl(loos, ~ pluck(., &quot;estimates&quot;)[&quot;elpd_loo&quot;, &quot;SE&quot;]) ) knitr::kable(elpd_stan_glm) formula n_coefs elpd elpd_se log(price) ~ log(carat) 2 -3931 190 log(price) ~ log(carat) + clarity 9 13713 188 log(price) ~ log(carat) + clarity + color_combined 12 27673 219 log(price) ~ log(carat) + clarity + color 15 30636 227 log(price) ~ log(carat) + clarity + color + cut 19 32267 246 Let’s again visualize the results. p &lt;- elpd_stan_glm %&gt;% ggplot(aes(n_coefs, elpd)) + geom_line() + geom_linerange(aes(ymin = elpd - elpd_se, ymax = elpd + elpd_se)) + geom_point() + geom_text( aes(label = formula), hjust = c(0, 0, 1, 1, 1), nudge_x = 0.2 * c(1, 1, -1, -1, -1) ) + scale_x_continuous( breaks = seq(min(elpd_stan_glm$n_coefs), max(elpd_stan_glm$n_coefs)), minor_breaks = NULL ) + labs( title = &quot;Bayesian LOO estimate of predictive performance&quot;, x = &quot;Number of model coefficients&quot;, y = &quot;Expected log predictive density&quot; ) p This plot looks very much like the cross-validation plot, though flipped vertically, since larger ELPD is better while smaller RSME is better. Let’s again zoom in on the three models with the best predictive performance. p + coord_cartesian(ylim = c(25000, NA)) At this scale, we can again see the error bars for one standard error on either side of the estimate. Using our rule of thumb of choosing the model with the lowest complexity whose estimated predictive performance is within one standard error of the best performance, we again choose the most complex model log(price) ~ log(carat) + clarity + color + cut . 5.3 Final model The classical and Bayesian approaches both led us to select the same function family. Here are the resulting models. fit_lm &lt;- lm(log(price) ~ log(carat) + clarity + color + cut, data = df) set.seed(983) fit_stan_glm &lt;- stan_glm( log(price) ~ log(carat) + clarity + color + cut, data = df, refresh = 0 ) We can compare the coefficients in the two models. coef(fit_lm) - coef(fit_stan_glm) #&gt; (Intercept) log(carat) claritySI2 claritySI1 clarityVS2 clarityVS1 #&gt; 8.47e-07 -1.76e-05 -2.13e-04 -1.73e-04 -1.27e-04 -2.33e-04 #&gt; clarityVVS2 clarityVVS1 clarityIF colorI colorH colorG #&gt; -1.44e-04 -2.14e-04 -9.50e-05 3.54e-05 5.40e-05 5.50e-05 #&gt; colorF colorE colorD cutGood cutVery Good cutPremium #&gt; 2.69e-05 6.76e-05 -1.47e-05 1.27e-04 1.04e-04 1.30e-04 #&gt; cutIdeal #&gt; 5.90e-05 The coefficients of the two models are very close. In the following, we’ll focus on the model from stan_glm(), since it provides more information. Let’s first look at the coefficients. print(fit_stan_glm, digits = 3) #&gt; stan_glm #&gt; family: gaussian [identity] #&gt; formula: log(price) ~ log(carat) + clarity + color + cut #&gt; observations: 53405 #&gt; predictors: 19 #&gt; ------ #&gt; Median MAD_SD #&gt; (Intercept) 7.381 0.006 #&gt; log(carat) 1.889 0.001 #&gt; claritySI2 0.400 0.005 #&gt; claritySI1 0.563 0.005 #&gt; clarityVS2 0.713 0.005 #&gt; clarityVS1 0.784 0.006 #&gt; clarityVVS2 0.919 0.006 #&gt; clarityVVS1 0.992 0.006 #&gt; clarityIF 1.087 0.006 #&gt; colorI 0.136 0.003 #&gt; colorH 0.259 0.003 #&gt; colorG 0.348 0.003 #&gt; colorF 0.413 0.003 #&gt; colorE 0.455 0.003 #&gt; colorD 0.509 0.003 #&gt; cutGood 0.078 0.004 #&gt; cutVery Good 0.115 0.004 #&gt; cutPremium 0.139 0.004 #&gt; cutIdeal 0.160 0.004 #&gt; #&gt; Auxiliary parameter(s): #&gt; Median MAD_SD #&gt; sigma 0.132 0.000 #&gt; #&gt; ------ #&gt; * For help interpreting the printed output see ?print.stanreg #&gt; * For info on the priors used see ?prior_summary.stanreg The (Intercept) coefficient implies that a one-carat diamond with the lowest quality clarity, color and cut would have a price of approximately $exp(7.381), or $1,605. This is close to the minimum price in the data of one-carat diamonds. df %&gt;% filter(near(carat, 1)) %&gt;% pull(price) %&gt;% min() #&gt; [1] 1681 The log(carat) coefficient implies that the power law for carat has a power of approximately 1.89. For the factors clarity, color, and cut, the levels are in the order of increasing quality. For example, IF is the highest quality of clarity, and D is the highest quality of color. We can see that the coefficients for the factor levels increase with increasing quality, as expected. The parameters for color level I and for all cut levels are comparable in size to sigma, an estimate of the standard deviation for the residuals, so we should not assume that these parameters are very reliable measures. In our EDA, we saw that diamonds with high-quality cut are common, and cut appears to have the least impact of the four Cs on price. By applying log() to both sides of the formula defining our function family, we obtain the function price = 1605 * g_clarity(clarity) * g_color(color) * g_cut(cut) * carat^1.89 where, for example, g_cut &lt;- function(cut) { case_when( cut == &quot;Fair&quot; ~ 1, cut == &quot;Good&quot; ~ 1.08, cut == &quot;Very Good&quot; ~ 1.12, cut == &quot;Premium&quot; ~ 1.15, cut == &quot;Ideal&quot; ~ 1.17 ) } Here each value is exp() applied to the parameter value. The omitted level for each factor, Fair for cut, is included in the intercept, so we use the value of exp(0) = 1. Let’s look at the multipliers associated with the factor levels. v &lt;- coef(fit_stan_glm) %&gt;% enframe() %&gt;% filter(str_detect(name, &quot;^c&quot;)) %&gt;% extract( col = name, into = c(&quot;var&quot;, &quot;level&quot;), regex = &quot;([a-z]+)([A-Z]+.*)&quot; ) %&gt;% mutate(value = exp(value)) v %&gt;% ggplot(aes(var, value)) + geom_hline(yintercept = 1, size = 1, color = &quot;white&quot;) + geom_point() + ggrepel::geom_text_repel( aes(label = level), nudge_x = 0.1, direction = &quot;y&quot;, hjust = 0, min.segment.length = 2 ) + coord_cartesian(ylim = c(0.75, NA)) + labs( title = &quot;Multipliers associated with different diamond quality levels&quot;, x = NULL, y = &quot;Multiplier&quot; ) As we saw in our EDA, clarity has the greatest impact on price, followed by color and cut. The model predicts that the price of diamonds with the best clarity, IF, will be 2.96 times the price for comparable diamonds of the worst clarity. The best color, D, has a multiplier of 1.66. And the best cut, Ideal, has a multiplier of only 1.17. The relatively small effect of cut could be seen during model selection. Adding cut to the model improved performance, but much less than adding clarity or color. Just as we did in Chapter 4, let’s check the residuals of the model by plotting the error ratios. v &lt;- df %&gt;% mutate( pred = predict(fit_stan_glm) %&gt;% exp(), error_ratio = price / pred ) v %&gt;% ggplot(aes(pred, error_ratio)) + geom_point(alpha = 0.01) + geom_hline(yintercept = 1, size = 1, color = &quot;white&quot;) + geom_smooth() + coord_cartesian(ylim = c(0, 2)) + scale_x_log10() + labs( title = &quot;Ratios of actual price / predicted price&quot;, x = &quot;Predicted price&quot;, y = &quot;Error ratio&quot; ) The first takeaway is that the error ratios are much closer to 1 for our final model than they were for the simple model we looked at in Chapter 4. For predicted prices less than about $800, we again see that the actual price is higher than the predicted price. For higher predicted prices, the smooth line is fairly close to 1. Let’s look at the distribution of error_ratio. quantile(v$error_ratio, probs = c(0.025, 0.05, 0.5, 0.95, 0.975)) #&gt; 2.5% 5% 50% 95% 97.5% #&gt; 0.776 0.810 0.999 1.249 1.306 The median error_ratio is very close to 1. Approximately 95% of the diamonds are within the range of 22% less and 31% more than the predictions. This is a much tighter range than with the simplest model. Many other factors, such as regional variation, affect price. So it is rather surprising that we can make such accurate predictions with only four predictor variables. 5.4 Summary In model selection: The greater the complexity of a model, the greater the likelihood that it will overfit its training data and not generalize and work well with new data. To avoid overfitting and to select models that optimize predictive performance, we need to estimate the predictive performance on new data. Classical modeling typically uses cross-validation to estimate predictive performance. Bayesian modeling can estimate predictive performance directly from the initial modeling process without having to fit additional models as with cross-validation. 5.5 To learn more Data modeling is a very large subject, and we’ve only provided a brief overview of the process and some useful tools and packages. Here are ways to learn more. As we’ve seen, in supervised learning where the goal is to make accurate predictions, it is important to understand your data, properly prepare your data, and then explore possible function families with which to model it. A good resource to learn more about this process is Kuhn and Johnson, Feature Engineering and Selection. We used some functions from the tidymodels ecosystem of R packages for modeling. A good source to go deeper is Kuhn and Silge, Tidy Modeling with R. A good introduction to Bayesian modeling is Gelman, Hill, and Vehtari, Regression and Other Stories. Gelman and Vehtari are active in the Stan project, and the book uses stan_glm() for both linear and other general linear models. Tidyverse implementations of examples from the book are available here. To go deeper into the workflow of Bayesian data analysis and modeling see Gelman, et al., Bayesian Workflow. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
